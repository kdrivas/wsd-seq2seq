{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import corenlp\n",
    "\n",
    "os.environ[\"CORENLP_HOME\"] = '/home/krivas/projects/neural-wsd/new_experiments/data/lib/stanford-corenlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(dirs):\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "def dependency_parse(filepath,  client, cp='', tokenize=True):\n",
    "    print('\\nDependency parsing ' + filepath)\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    filepre = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    parentpath = os.path.join(dirpath, filepre + '.parents')\n",
    "    deps = []\n",
    "    with open(filepath) as file:\n",
    "        for line in tqdm(file, total=file.tell()):\n",
    "            temp = client.dependency_parse(line)\n",
    "            temp = list(map(lambda x: [int(x[1]), int(x[2])], temp))\n",
    "            temp = list(itertools.chain(*temp))\n",
    "            deps.append(temp)\n",
    "    np.save(parentpath, np.array(deps))\n",
    "\n",
    "def split(filepath, dst_dir, client):\n",
    "    with open(filepath) as datafile, \\\n",
    "            open(os.path.join(dst_dir, 'a.txt'), 'w') as afile, \\\n",
    "            open(os.path.join(dst_dir, 'b.txt'), 'w') as bfile:\n",
    "        datafile.readline()\n",
    "        for line in tqdm(datafile):\n",
    "            a, b = line.strip().split('\\t')\n",
    "\n",
    "            ann = client.annotate(a)\n",
    "            s = ' '.join([w.word for w in ann.sentence[0].token])\n",
    "            afile.write(a + '\\n')\n",
    "                \n",
    "            ann = client.annotate(b)\n",
    "            s = ' '.join([w.word for w in ann.sentence[0].token])\n",
    "            bfile.write(b + '\\n')\n",
    "\n",
    "def parse(dirpath, client, cp=''):\n",
    "    dependency_parse(os.path.join(dirpath, 'a.txt'), client, cp=cp, tokenize=True)\n",
    "    dependency_parse(os.path.join(dirpath, 'b.txt'), client, cp=cp, tokenize=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('=' * 80)\n",
    "    print('Preprocessing dataset')\n",
    "    print('=' * 80)\n",
    "\n",
    "    base_dir = ''\n",
    "    data_dir = os.path.join(base_dir, 'data')\n",
    "    all_dir = os.path.join(data_dir, 'translation/all_data')\n",
    "    lib_dir = os.path.join(base_dir, 'lib')\n",
    "    train_dir = os.path.join(data_dir, 'translation/train')\n",
    "    #dev_dir = os.path.join(data_dir, 'translation/dev')\n",
    "    #test_dir = os.path.join(data_dir, 'translation/test')\n",
    "    make_dirs([train_dir])\n",
    "\n",
    "    # java classpath for calling Stanford parser\n",
    "    classpath = ':'.join([\n",
    "        lib_dir,\n",
    "        os.path.join(lib_dir, 'stanford-parser/stanford-parser.jar'),\n",
    "        os.path.join(lib_dir, 'stanford-parser/stanford-parser-3.5.1-models.jar')])\n",
    "\n",
    "    # split into separate files\n",
    "    client = corenlp.CoreNLPClient(annotators=\"tokenize ssplit\".split())\n",
    "    print('create client')\n",
    "    split(os.path.join(all_dir, 'en-spa.txt'), train_dir, client)\n",
    "    #split(os.path.join(all_dir, 'SICK_trial.txt'), dev_dir)\n",
    "    #split(os.path.join(all_dir, 'SICK_test_annotated.txt'), test_dir)\n",
    "\n",
    "    # parse sentences\n",
    "    client = StanfordCoreNLP(r'data/lib/stanford-corenlp')\n",
    "    parse(train_dir, client, cp=classpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = corenlp.CoreNLPClient(annotators=\"tokenize ssplit\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermanentlyFailedException",
     "evalue": "Timed out waiting for service to come alive.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermanentlyFailedException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-153a64a779a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# and communicate with the server to annotate the sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcorenlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tokenize ssplit\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://localhost:9010'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# You can access annotations using ann.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/corenlp/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, properties)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;34m'serializer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             })\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mparseFromDelimitedString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/corenlp/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/corenlp/client.py\u001b[0m in \u001b[0;36mensure_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPermanentlyFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timed out waiting for service to come alive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# At this point we are guaranteed that the service is alive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermanentlyFailedException\u001b[0m: Timed out waiting for service to come alive."
     ]
    }
   ],
   "source": [
    "text = \"Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\"\n",
    "\n",
    "# We assume that you've downloaded Stanford CoreNLP and defined an environment\n",
    "# variable $CORENLP_HOME that points to the unzipped directory.\n",
    "# The code below will launch StanfordCoreNLPServer in the background\n",
    "# and communicate with the server to annotate the sentence.\n",
    "with corenlp.CoreNLPClient(annotators=\"tokenize ssplit\".split(), timeout=10000, endpoint='http://localhost:9010') as client:\n",
    "  ann = client.annotate(text)\n",
    "\n",
    "# You can access annotations using ann.\n",
    "sentence = ann.sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<corenlp.client.CoreNLPClient at 0x7f2630df1e80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nlp = StanfordCoreNLP(r'data/lib/stanford-corenlp')\n",
    "\n",
    "sentence = 'Guangdong University of Foreign Studies is located in Guangzhou.'\n",
    "\n",
    "a = np.array(nlp.dependency_parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.loadtxt('f.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   7.],\n",
       "       [  2.,   1.],\n",
       "       [  7.,   2.],\n",
       "       [  5.,   3.],\n",
       "       [  5.,   4.],\n",
       "       [  2.,   5.],\n",
       "       [  7.,   6.],\n",
       "       [  9.,   8.],\n",
       "       [  7.,   9.],\n",
       "       [  7.,  10.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 7],\n",
       " [2, 1],\n",
       " [7, 2],\n",
       " [5, 3],\n",
       " [5, 4],\n",
       " [2, 5],\n",
       " [7, 6],\n",
       " [9, 8],\n",
       " [7, 9],\n",
       " [7, 10]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x:[int(x[1]), int(x[2])], a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing script for SICK data.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Preprocessing SICK dataset\n",
      "================================================================================\n",
      "\n",
      "Dependency parsing data/translation/train/a.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115244it [16:37, 115.58it/s]\n",
      "25it [00:00, 241.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency parsing data/translation/train/b.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115244it [27:51, 68.94it/s]\n"
     ]
    }
   ],
   "source": [
    "    print('=' * 80)\n",
    "    print('Preprocessing SICK dataset')\n",
    "    print('=' * 80)\n",
    "\n",
    "    base_dir = ''\n",
    "    data_dir = os.path.join(base_dir, 'data')\n",
    "    all_dir = os.path.join(data_dir, 'translation/all_data')\n",
    "    lib_dir = os.path.join(base_dir, 'lib')\n",
    "    train_dir = os.path.join(data_dir, 'translation/train')\n",
    "    #dev_dir = os.path.join(data_dir, 'translation/dev')\n",
    "    #test_dir = os.path.join(data_dir, 'translation/test')\n",
    "    make_dirs([train_dir])\n",
    "\n",
    "    # java classpath for calling Stanford parser\n",
    "    classpath = ':'.join([\n",
    "        lib_dir,\n",
    "        os.path.join(lib_dir, 'stanford-parser/stanford-parser.jar'),\n",
    "        os.path.join(lib_dir, 'stanford-parser/stanford-parser-3.5.1-models.jar')])\n",
    "\n",
    "    # split into separate files\n",
    "    split(os.path.join(all_dir, 'en-spa.txt'), train_dir)\n",
    "    #split(os.path.join(all_dir, 'SICK_trial.txt'), dev_dir)\n",
    "    #split(os.path.join(all_dir, 'SICK_test_annotated.txt'), test_dir)\n",
    "\n",
    "    # parse sentences\n",
    "    parse(train_dir, cp=classpath)\n",
    "    #parse(dev_dir, cp=classpath)\n",
    "    #parse(test_dir, cp=classpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corenlp\n",
    "text = \"Chris wrote a simple gotta sentence that he parsed with Stanford CoreNLP.\"\n",
    "\n",
    "# We assume that you've downloaded Stanford CoreNLP and defined an environment\n",
    "# variable $CORENLP_HOME that points to the unzipped directory.\n",
    "# The code below will launch StanfordCoreNLPServer in the background\n",
    "# and communicate with the server to annotate the sentence.\n",
    "ann = client.annotate(text)\n",
    "s = ' '.join([w.word for w in ann.sentence[0].token])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "client =  corenlp.CoreNLPClient(annotators=\"tokenize ssplit\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[token {\n",
       "  word: \"Chris\"\n",
       "  value: \"Chris\"\n",
       "  before: \"\"\n",
       "  after: \" \"\n",
       "  originalText: \"Chris\"\n",
       "  beginChar: 0\n",
       "  endChar: 5\n",
       "  tokenBeginIndex: 0\n",
       "  tokenEndIndex: 1\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"wrote\"\n",
       "  value: \"wrote\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"wrote\"\n",
       "  beginChar: 6\n",
       "  endChar: 11\n",
       "  tokenBeginIndex: 1\n",
       "  tokenEndIndex: 2\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"a\"\n",
       "  value: \"a\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"a\"\n",
       "  beginChar: 12\n",
       "  endChar: 13\n",
       "  tokenBeginIndex: 2\n",
       "  tokenEndIndex: 3\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"simple\"\n",
       "  value: \"simple\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"simple\"\n",
       "  beginChar: 14\n",
       "  endChar: 20\n",
       "  tokenBeginIndex: 3\n",
       "  tokenEndIndex: 4\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"got\"\n",
       "  value: \"got\"\n",
       "  before: \" \"\n",
       "  after: \"\"\n",
       "  originalText: \"got\"\n",
       "  beginChar: 21\n",
       "  endChar: 24\n",
       "  tokenBeginIndex: 4\n",
       "  tokenEndIndex: 5\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"ta\"\n",
       "  value: \"ta\"\n",
       "  before: \"\"\n",
       "  after: \" \"\n",
       "  originalText: \"ta\"\n",
       "  beginChar: 24\n",
       "  endChar: 26\n",
       "  tokenBeginIndex: 5\n",
       "  tokenEndIndex: 6\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"sentence\"\n",
       "  value: \"sentence\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"sentence\"\n",
       "  beginChar: 27\n",
       "  endChar: 35\n",
       "  tokenBeginIndex: 6\n",
       "  tokenEndIndex: 7\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"that\"\n",
       "  value: \"that\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"that\"\n",
       "  beginChar: 36\n",
       "  endChar: 40\n",
       "  tokenBeginIndex: 7\n",
       "  tokenEndIndex: 8\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"he\"\n",
       "  value: \"he\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"he\"\n",
       "  beginChar: 41\n",
       "  endChar: 43\n",
       "  tokenBeginIndex: 8\n",
       "  tokenEndIndex: 9\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"parsed\"\n",
       "  value: \"parsed\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"parsed\"\n",
       "  beginChar: 44\n",
       "  endChar: 50\n",
       "  tokenBeginIndex: 9\n",
       "  tokenEndIndex: 10\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"with\"\n",
       "  value: \"with\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"with\"\n",
       "  beginChar: 51\n",
       "  endChar: 55\n",
       "  tokenBeginIndex: 10\n",
       "  tokenEndIndex: 11\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"Stanford\"\n",
       "  value: \"Stanford\"\n",
       "  before: \" \"\n",
       "  after: \" \"\n",
       "  originalText: \"Stanford\"\n",
       "  beginChar: 56\n",
       "  endChar: 64\n",
       "  tokenBeginIndex: 11\n",
       "  tokenEndIndex: 12\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \"CoreNLP\"\n",
       "  value: \"CoreNLP\"\n",
       "  before: \" \"\n",
       "  after: \"\"\n",
       "  originalText: \"CoreNLP\"\n",
       "  beginChar: 65\n",
       "  endChar: 72\n",
       "  tokenBeginIndex: 12\n",
       "  tokenEndIndex: 13\n",
       "  hasXmlContext: false\n",
       "}\n",
       "token {\n",
       "  word: \".\"\n",
       "  value: \".\"\n",
       "  before: \"\"\n",
       "  after: \"\"\n",
       "  originalText: \".\"\n",
       "  beginChar: 72\n",
       "  endChar: 73\n",
       "  tokenBeginIndex: 13\n",
       "  tokenEndIndex: 14\n",
       "  hasXmlContext: false\n",
       "}\n",
       "tokenOffsetBegin: 0\n",
       "tokenOffsetEnd: 14\n",
       "sentenceIndex: 0\n",
       "characterOffsetBegin: 0\n",
       "characterOffsetEnd: 73\n",
       "hasRelationAnnotations: false\n",
       "hasNumerizedTokensAnnotation: false\n",
       "]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can access annotations using ann.\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = '/home/krivas/projects/neural-wsd/new_experiments/data/lib/stanford-corenlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "com = 'java -cp \"/home/krivas/projects/neural-wsd/new_experiments/data/lib/stanford-corenlp/stanford-corenlp-3.9.2-models.jar:/home/krivas/projects/neural-wsd/new_experiments/data/lib/stanford-corenlp/stanford-corenlp-3.9.2.jar\"  -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize -file input.txt -outputFormat json'\n",
    "output = subprocess.Popen(['ls', '-la'], stdout=subprocess.PIPE)\n",
    "response=output.communicate()[0]\n",
    "print (response.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115244,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [1, 2]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
