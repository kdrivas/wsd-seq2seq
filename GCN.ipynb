{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'../stanford-corenlp-full-2017-06-09')\n",
    "\n",
    "sentence = 'which you step on to activate it'\n",
    "de = nlp.dependency_parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency Parsing: [('ROOT', 0, 3), ('dobj', 3, 1), ('nsubj', 3, 2), ('mark', 6, 4), ('mark', 6, 5), ('advcl', 3, 6), ('dobj', 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "print ('Dependency Parsing:', de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dep_sentences = []\n",
    "dep_sentences.append(de)\n",
    "dep_sentences.append(de)\n",
    "dep_sentences.append(de)\n",
    "dep_sentences.append(de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_DEP_LABELS = ['ROOT', 'DOBJ','ADV', 'ADV-GAP', 'AMOD', 'APPO', 'BNF', 'CONJ', 'COORD', 'DEP',\n",
    "               'DEP-GAP', 'DIR', 'DIR-GAP', 'DIR-OPRD', 'DIR-PRD', 'DTV', 'EXT',\n",
    "               'EXT-GAP', 'EXTR', 'GAP-LGS', 'GAP-LOC', 'GAP-LOC-PRD', 'GAP-MNR',\n",
    "               'GAP-NMOD', 'GAP-OBJ', 'GAP-OPRD', 'GAP-PMOD', 'GAP-PRD', 'GAP-PRP',\n",
    "               'GAP-SBJ', 'GAP-TMP', 'GAP-VC', 'HMOD', 'HYPH', 'IM', 'LGS', 'LOC',\n",
    "               'LOC-OPRD', 'LOC-PRD', 'LOC-TMP', 'MNR', 'MNR-PRD', 'MNR-TMP', 'NAME',\n",
    "               'NMOD', 'NSUBJ','OBJ', 'OPRD', 'P', 'PMOD', 'POSTHON', 'PRD', 'PRD-PRP',\n",
    "               'PRD-TMP', 'PRN', 'PRP', 'PRT', 'PUT', 'SBJ', 'SUB', 'SUFFIX',\n",
    "                'TITLE', 'TMP', 'VC', 'VOC']\n",
    "\n",
    "\n",
    "_DEP_LABELS_DICT = {label:ix for ix, label in enumerate(_DEP_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_DEP_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings batch variables\n",
    "\n",
    "SEQ_LEN = len(sentence)\n",
    "BATCH_SIZE = len(dep_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize adjancencies matrixes\n",
    "\n",
    "adj_arc_in = np.zeros((BATCH_SIZE* SEQ_LEN, 2), dtype='int32')\n",
    "adj_lab_in = np.zeros((BATCH_SIZE* SEQ_LEN), dtype='int32')\n",
    "\n",
    "adj_arc_out = np.zeros((BATCH_SIZE * SEQ_LEN, 2), dtype='int32')\n",
    "adj_lab_out = np.zeros((BATCH_SIZE * SEQ_LEN), dtype='int32')\n",
    "\n",
    "#Initialize mask matrix\n",
    "\n",
    "mask_in = np.zeros((BATCH_SIZE * SEQ_LEN), dtype='float32')\n",
    "mask_out = np.zeros((BATCH_SIZE * SEQ_LEN), dtype='float32')\n",
    "\n",
    "mask_loop = np.ones((BATCH_SIZE * SEQ_LEN, 1), dtype='float32')\n",
    "\n",
    "#Get adjacency matrix for incoming and outgoing arcs\n",
    "for idx_sentence, dep_sentence in enumerate(dep_sentences):\n",
    "    for idx_arc, arc in enumerate(dep_sentence):\n",
    "        if(arc[0] != 'ROOT') and arc[0].upper() in _DEP_LABELS:\n",
    "            #get index of words in the sentence\n",
    "            arc_1 = int(arc[1]) - 1\n",
    "            arc_2 = int(arc[2]) - 1\n",
    "\n",
    "            idx = (idx_arc) + idx_sentence * BATCH_LEN\n",
    "            \n",
    "            #Make adjacency matrix for incoming arcs\n",
    "            adj_arc_in[idx] = np.array([idx_sentence, arc_2]) \n",
    "            adj_lab_in[idx] = np.array([_DEP_LABELS_DICT[arc[0].upper()]]) \n",
    "            \n",
    "            #Setting mask to consider that index\n",
    "            mask_in[idx] = 1\n",
    "\n",
    "            #Make adjacency matrix for outgoing arcs\n",
    "            adj_arc_out[idx] = np.array([idx_sentence, arc_1])   \n",
    "            adj_lab_out[idx] = np.array([_DEP_LABELS_DICT[arc[0].upper()]])\n",
    "            \n",
    "            #Setting mask to consider that index\n",
    "            mask_out[idx] = 1\n",
    "\n",
    "adj_arc_in = torch.LongTensor(np.transpose(adj_arc_in)) \n",
    "adj_arc_out = torch.LongTensor(np.transpose(adj_arc_out))\n",
    "\n",
    "adj_lab_in = Variable(torch.LongTensor(adj_lab_in))\n",
    "adj_lab_out = Variable(torch.LongTensor(adj_lab_out))\n",
    "\n",
    "mask_in = Variable(torch.FloatTensor(mask_in.reshape((BATCH_SIZE * SEQ_LEN, 1))))\n",
    "mask_out = Variable(torch.FloatTensor(mask_out.reshape((BATCH_SIZE * SEQ_LEN, 1))))\n",
    "mask_loop = Variable(torch.FloatTensor(mask_loop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SintacticGCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_units,\n",
    "                 num_labels,\n",
    "                 dropout = 0.,\n",
    "                 in_arcs = True,\n",
    "                 out_arcs = True,\n",
    "                 batch_first = False):       \n",
    "        super(SintacticGCN, self).__init__()      \n",
    "\n",
    "        self.in_arcs = in_arcs\n",
    "        self.out_arcs = out_arcs\n",
    "        \n",
    "        self.retain = 1. - dropout\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_units = num_units\n",
    "        self.num_labels = num_labels\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        if in_arcs:\n",
    "            self.V_in = Variable(torch.FloatTensor(self.num_inputs, self.num_units))\n",
    "            nn.init.xavier_normal(self.V_in)\n",
    "            \n",
    "            self.b_in = Variable(torch.FloatTensor(num_labels, self.num_units))\n",
    "            nn.init.constant(self.b_in, 0)\n",
    "            \n",
    "            self.V_in_gate = Variable(torch.FloatTensor(self.num_inputs, 1))\n",
    "            nn.init.uniform(self.V_in_gate)\n",
    "            \n",
    "            self.b_in_gate = Variable(torch.FloatTensor(num_labels, 1))\n",
    "            nn.init.constant(self.b_in_gate, 1)\n",
    "\n",
    "        if out_arcs:\n",
    "            self.V_out = Variable(torch.FloatTensor(self.num_inputs, self.num_units))\n",
    "            nn.init.xavier_normal(self.V_out)\n",
    "            \n",
    "            self.b_out = Variable(torch.FloatTensor(num_labels, self.num_units))\n",
    "            nn.init.constant(self.b_in, 0)\n",
    "            \n",
    "            self.V_out_gate = Variable(torch.FloatTensor(self.num_inputs, 1))\n",
    "            nn.init.uniform(self.V_out_gate)\n",
    "            \n",
    "            self.b_out_gate = Variable(torch.FloatTensor(num_labels, 1))\n",
    "            nn.init.constant(self.b_out_gate, 1)\n",
    "        \n",
    "        self.W_self_loop = Variable(torch.FloatTensor(self.num_inputs, self.num_units))\n",
    "        nn.init.xavier_normal(self.W_self_loop)        \n",
    "        \n",
    "        self.W_self_loop_gate = Variable(torch.FloatTensor(self.num_inputs, 1))\n",
    "        nn.init.uniform(self.W_self_loop_gate)\n",
    "\n",
    "    def forward(self, encoder_outputs,\n",
    "                 arc_tensor_in, arc_tensor_out,\n",
    "                 label_tensor_in, label_tensor_out,\n",
    "                 mask_in, mask_out,  # batch* t, degree\n",
    "                 mask_loop):\n",
    "\n",
    "        if(not self.batch_first):\n",
    "            encoder_outputs = encoder_outputs.permute(1, 0, 2).contiguous()\n",
    "        \n",
    "        batch_size, seq_len, _ = encoder_outputs.shape\n",
    "        max_degree = 1\n",
    "        input_ = encoder_outputs.view((batch_size * seq_len , self.num_inputs))  # [b* t, h]\n",
    "        \n",
    "        if self.in_arcs:\n",
    "            input_in = torch.mm(input_, self.V_in)  # [b* t, h] * [h,h] = [b*t, h]\n",
    "            second_in = self.b_in.index_select(0, label_tensor_in)  # [b* t* 1, h]\n",
    "            in_ = (input_in + second_in).view((batch_size, seq_len, 1, self.num_units))\n",
    "\n",
    "            # compute gate weights\n",
    "            input_in_gate = torch.mm(input_, self.V_in_gate)  # [b* t, h] * [h,h] = [b*t, h]\n",
    "            second_in_gate = self.b_in_gate.index_select(0, label_tensor_in)\n",
    "            in_gate = (input_in_gate + second_in_gate).view((batch_size, seq_len, 1))\n",
    "\n",
    "            max_degree += 1\n",
    "            \n",
    "        if self.out_arcs:           \n",
    "            input_out = torch.mm(input_, self.V_out)  # [b* t, h] * [h,h] = [b* t, h]\n",
    "            second_out = self.b_out.index_select(0, label_tensor_out)     \n",
    "            \n",
    "            degr = int(input_out.shape[0] / batch_size / seq_len)\n",
    "            max_degree += degr\n",
    "\n",
    "            out_ = (input_out + second_out).view((batch_size, seq_len, degr, self.num_units))\n",
    "\n",
    "            # compute gate weights\n",
    "            input_out_gate = torch.mm(input_, self.V_out_gate)  # [b* t, h] * [h,h] = [b* t, h]\n",
    "            second_out_gate = self.b_out_gate.index_select(0, label_tensor_out)\n",
    "            out_gate = (input_out_gate + second_out_gate).view((batch_size, seq_len, degr))\n",
    "       \n",
    "        same_input = torch.mm(encoder_outputs.view(-1,encoder_outputs.size(2)), self.W_self_loop).\\\n",
    "                        view(encoder_outputs.size(0), encoder_outputs.size(1), -1)\n",
    "        same_input = same_input.view(encoder_outputs.size(0), encoder_outputs.size(1), 1, self.W_self_loop.size(1))\n",
    "        \n",
    "        same_input_gate = torch.mm(encoder_outputs.view(-1, encoder_outputs.size(2)), self.W_self_loop_gate)\\\n",
    "                                .view(encoder_outputs.size(0), encoder_outputs.size(1), -1)\n",
    "\n",
    "        if self.in_arcs and self.out_arcs:\n",
    "            potentials = torch.cat((in_, out_, same_input), dim=2)  # [b, t,  mxdeg, h]         \n",
    "            potentials_gate = torch.cat((in_gate, out_gate, same_input_gate), dim=2)  # [b, t,  mxdeg, h]\n",
    "            mask_soft = torch.cat((mask_in, mask_out, mask_loop), dim=1)  # [b* t, mxdeg]\n",
    "        elif self.out_arcs:\n",
    "            potentials = torch.cat((out_, same_input), dim=2)  # [b, t,  2*mxdeg+1, h]\n",
    "            potentials_gate = torch.cat((out_gate, same_input_gate), dim=2)  # [b, t,  mxdeg, h]\n",
    "            mask_soft = torch.cat((mask_out, mask_loop), dim=1)  # [b* t, mxdeg]\n",
    "        elif self.in_arcs:\n",
    "            potentials = torch.cat((in_, same_input), dim=2)  # [b, t,  2*mxdeg+1, h]\n",
    "            potentials_gate = torch.cat((in_gate, same_input_gate), dim=2)  # [b, t,  mxdeg, h]\n",
    "            mask_soft = torch.cat((mask_in, mask_loop), dim=1)  # [b* t, mxdeg]\n",
    "\n",
    "        potentials_ = potentials.permute(3, 0, 1, 2).contiguous()  # [h, b, t, mxdeg]\n",
    "        potentials_resh = potentials_.view((self.num_units,\n",
    "                                               batch_size * seq_len,\n",
    "                                               max_degree))  # [h, b * t, mxdeg]\n",
    "\n",
    "        potentials_r = potentials_gate.view((batch_size * seq_len,\n",
    "                                                  max_degree))  # [h, b * t, mxdeg]\n",
    "        # calculate the gate\n",
    "        probs_det_ = self.sigmoid(potentials_r) * mask_soft  # [b * t, mxdeg]\n",
    "        potentials_masked = potentials_resh * mask_soft * probs_det_  # [h, b * t, mxdeg]\n",
    "\n",
    "        \n",
    "        #if self.retain == 1 or deterministic:\n",
    "        #    pass\n",
    "        #else:\n",
    "        #    drop_mask = self._srng.binomial(potentials_resh.shape[1:], p=self.retain, dtype=input.dtype)\n",
    "        #    potentials_masked /= self.retain\n",
    "        #    potentials_masked *= drop_mask\n",
    "\n",
    "        potentials_masked_ = potentials_masked.sum(dim=2)  # [h, b * t]\n",
    "        potentials_masked_ = self.relu(potentials_masked_)\n",
    "\n",
    "        result_ = potentials_masked_.permute(1, 0).contiguous()   # [b * t, h]\n",
    "        result_ = result_.view((batch_size, seq_len, self.num_units))  # [ b, t, h]\n",
    "\n",
    "        return result_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 10])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 10)\n",
    "\n",
    "batch = Variable(torch.ones(SEQ_LEN, BATCH_SIZE)).type(torch.LongTensor)\n",
    "embed = embedding(batch)\n",
    "\n",
    "print(embed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.1744  0.6544  0.0000  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "\n",
       "(2 ,.,.) = \n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "\n",
       "(3 ,.,.) = \n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "  0.0257  0.0220  0.1397  0.0000  0.0000\n",
       "[torch.FloatTensor of size 4x32x5]"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn = SintacticGCN(10, 5, 65)\n",
    "gcn(embed,\n",
    "                 adj_arc_in, adj_arc_out,\n",
    "                 adj_lab_in, adj_lab_out,\n",
    "                 mask_in, mask_out,  \n",
    "                 mask_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
