{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import construct_pairs\n",
    "from Preprocessing import prepare_data\n",
    "from Preprocessing import load_senses\n",
    "from Preprocessing import random_batch\n",
    "\n",
    "from Evaluator import Evaluator\n",
    "\n",
    "from Model import Encoder_rnn\n",
    "from Model import Global_attn\n",
    "from Model import Attn_decoder_rnn\n",
    "from Model import train\n",
    "\n",
    "from Utils import time_since\n",
    "from Utils import load_json\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 600\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8642, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_train = 'data_train_con_poda.json'\n",
    "pairs_train = load_json(file_path_train)\n",
    "pairs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4252, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_test = 'data_test_con_poda.json'\n",
    "pairs_test = load_json(file_path_test)\n",
    "pairs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pairs 8642\n",
      "Filtered to 8642 pairs\n",
      "Indexing words...\n",
      "Indexed 17051 words in input language, 17844 words in output\n"
     ]
    }
   ],
   "source": [
    "sentence, sense = prepare_data(pairs_train, pairs_test, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_senses_test = load_senses('corpus/test/EnglishLS.test.key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 5000\n",
    "epoch = 0\n",
    "print_every = 10\n",
    "validate_loss_every = 50\n",
    "validate_acc_every = 2 * validate_loss_every\n",
    "epoch_finish_teacher_forcing = 3500\n",
    "tf_ratio_arr = np.linspace(1.0, 0.0, epoch_finish_teacher_forcing)\n",
    "tf_limit = 0.5\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder_rnn(sentence.n_words, hidden_size, n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "decoder = Attn_decoder_rnn('general', hidden_size, sense.n_words, 2 * n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "validation_losses = []\n",
    "validation_acc = []\n",
    "\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6m 28s (- 3231m 29s) (10 0.20%) train_loss: 4.6649 \n",
      "11m 30s (- 2865m 33s) (20 0.40%) train_loss: 2.8472 \n",
      "17m 44s (- 2940m 20s) (30 0.60%) train_loss: 2.4973 \n",
      "23m 5s (- 2864m 11s) (40 0.80%) train_loss: 2.5172 \n",
      "28m 14s (- 2795m 13s) (50 1.00%) train_loss: 2.3393 - val_loss: 3.2474 \n",
      "32m 38s (- 2687m 40s) (60 1.20%) train_loss: 2.5140 \n",
      "37m 26s (- 2636m 32s) (70 1.40%) train_loss: 2.4002 \n",
      "41m 47s (- 2569m 41s) (80 1.60%) train_loss: 2.4846 \n",
      "46m 17s (- 2525m 29s) (90 1.80%) train_loss: 2.3986 \n",
      "51m 9s (- 2506m 51s) (100 2.00%) train_loss: 2.3130 - val_loss: 2.6925 "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Lang' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ad4ef73895f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidate_acc_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers_senses_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mvalidation_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/wsd-v2/Evaluator.py\u001b[0m in \u001b[0;36mevaluate_acc\u001b[0;34m(self, id_pairs, pairs, answer_senses, k_beams)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/wsd-v2/Evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, sentence, k_beams)\u001b[0m\n\u001b[1;32m     74\u001b[0m                         \u001b[0mtop_beams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_beam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                         \u001b[0mnew_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoded_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Lang' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths, _ = random_batch(sentence, sense, batch_size, pairs_train, USE_CUDA)\n",
    "    \n",
    "    if epoch < epoch_finish_teacher_forcing:\n",
    "        tf_ratio = tf_ratio_arr[epoch]\n",
    "    else:\n",
    "        tf_ratio = 0\n",
    "    \n",
    "    if tf_ratio < tf_limit:\n",
    "        use_tf = False\n",
    "    else:\n",
    "        use_tf = True\n",
    "    \n",
    "    # Run the train function\n",
    "    loss = train(sentence, sense,\n",
    "        input_batches, input_lengths, target_batches, target_lengths, batch_size,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer, criterion, use_tf, MAX_LENGTH, clip, USE_CUDA=USE_CUDA)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        plot_losses.append(loss)\n",
    "        \n",
    "        print(f'{time_since(start, epoch / n_epochs)} ({epoch} {epoch / n_epochs * 100:.2f}%) train_loss: {print_loss_avg:.4f}', end=' ')\n",
    "\n",
    "    if epoch % validate_loss_every == 0:\n",
    "        input_batches, input_lengths, target_batches, target_lengths, id_pairs = random_batch(sentence, sense,\n",
    "                batch_size, pairs_test, USE_CUDA)\n",
    "    \n",
    "        eval_loss = train(sentence, sense, input_batches, input_lengths, target_batches, target_lengths,batch_size,\\\n",
    "                     encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\\\n",
    "                          0, MAX_LENGTH, train=False, USE_CUDA=USE_CUDA)\n",
    "        validation_losses.append(eval_loss)\n",
    "        \n",
    "        print(f'- val_loss: {eval_loss:.4f}', end=' ')\n",
    "        \n",
    "    if epoch % validate_acc_every == 0:\n",
    "        evaluator = Evaluator(encoder, decoder, sentence, sense, MAX_LENGTH, USE_CUDA)\n",
    "        eval_acc = evaluator.evaluate_acc(id_pairs, pairs_test, answers_senses_test)\n",
    "        \n",
    "        validation_acc.append(eval_acc)\n",
    "        \n",
    "        print(f'- val_acc: {eval_acc:.4f}', end='')\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
