{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_batches\n",
    "from data import prepare_data\n",
    "from data import data_to_index\n",
    "from data import DEP_LABELS\n",
    "from data import random_batch\n",
    "\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder_luong\n",
    "from model.gcn import Gcn\n",
    "\n",
    "from BLEU import BLEU\n",
    "\n",
    "from utils import time_since\n",
    "\n",
    "from evaluator import Evaluator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "#from validation import Evaluator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "MAX_LENGTH = 100\n",
    "DIR_FILES = 'data/translation/train/'\n",
    "DIR_RESULTS = 'results/step_1'\n",
    "SPLIT_TRAIN = 0.7\n",
    "SPLIT_VALID = 0.15\n",
    "# The rest is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 115244 sentence pairs\n",
      "Filtered to 84144 pairs\n",
      "Creating vocab...\n",
      "Creating matrixes...\n",
      "Indexed 12330 words in input language, 21913 words in output\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, input_trees, _, pairs = prepare_data('eng', 'esp', dir=DIR_FILES, return_trees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrixes = input_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs_train = np.array(pairs[:60000])\n",
    "pairs_test = np.array(pairs[60000:])\n",
    "\n",
    "matrixes_train = np.array(input_matrixes[:60000])\n",
    "matrixes_test = np.array(input_matrixes[60000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_batches, target_batches, input_matrixes,\\\n",
    "          encoder, decoder, gcn, criterion, batch_ix, train=True):\n",
    "    \n",
    "    if train and (batch_ix % batch_size) == 0:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        gcn_optimizer.zero_grad()\n",
    "        \n",
    "    loss = 0\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(1)\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, encoder_hidden)\n",
    "\n",
    "    encoder_outputs = nn.LeakyReLU()(gcn(encoder_outputs.squeeze(1), input_matrixes).unsqueeze(1))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    #print(encoder_outputs.shape, state.shape)\n",
    "    \n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))   \n",
    "    #decoder_hidden = encoder_hidden\n",
    "    # set the start of the sentences of the batch\n",
    "    decoder_input = torch.LongTensor([input_lang.vocab.stoi['<sos>']] * 1)\n",
    "\n",
    "    # store the decoder outputs to estimate the loss\n",
    "    all_decoder_outputs = Variable(torch.zeros(target_batches.size()[0], 1, len(output_lang.vocab.stoi)))\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        decoder_context = decoder_context.cuda()  \n",
    "    \n",
    "    if train:\n",
    "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    else:\n",
    "        use_teacher_forcing = False\n",
    "    \n",
    "    if use_teacher_forcing:        \n",
    "        # Use targets as inputs\n",
    "        for di in range(target_batches.shape[0]):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            decoder_input = target_batches[di]\n",
    "    else:        \n",
    "        # Use decoder output as inputs\n",
    "        for di in range(target_batches.shape[0]): \n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs) \n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            \n",
    "            # Greedy approach, take the word with highest probability\n",
    "            topv, topi = decoder_output.data.topk(1)            \n",
    "            decoder_input = Variable(torch.LongTensor(topi.cpu()).squeeze(dim=0))\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    loss = nn.NLLLoss()(all_decoder_outputs.view(-1, decoder.output_size), target_batches.contiguous().view(-1))          \n",
    "    \n",
    "    if train and (batch_ix % batch_size) == 0:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(gcn.parameters(), clip)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        gcn_optimizer.step()\n",
    "    elif train:\n",
    "        loss.backward()\n",
    "    \n",
    "    return loss.item() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_model = 'general'\n",
    "hidden_size = 512\n",
    "emb_size = 300\n",
    "n_layers = 2\n",
    "dropout_p = 0.1\n",
    "seed = 12\n",
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class Gcn(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Gcn, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, input, adj):\n",
    "        # input: (seq_len x in_features)\n",
    "        # adj: (seq_len x seq_len)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'lala' in ['alala', 'hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(input_lang.vocab.stoi), hidden_size, emb_size, n_layers, dropout_p, input_lang, USE_CUDA)\n",
    "decoder = Decoder_luong(attn_model, hidden_size, len(output_lang.vocab.stoi), emb_size, 2 * n_layers, dropout_p, output_lang, USE_CUDA)\n",
    "gcn = Gcn(hidden_size, hidden_size)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    gcn = gcn.cuda()\n",
    "    \n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, encoder.parameters()), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, decoder.parameters()), lr=learning_rate)\n",
    "gcn_optimizer = optim.Adam(gcn.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "validation_bleu = []\n",
    "\n",
    "plot_every = 5\n",
    "print_every = 5\n",
    "validate_loss_every = 25\n",
    "best_bleu = 0\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729m 16s (- 13856m 15s) (1 1%) 8.9886\n",
      "729m 44s (- 13865m 2s) (1 2%) 7.8141\n",
      "730m 12s (- 13874m 0s) (1 3%) 7.6746\n",
      "730m 40s (- 13882m 54s) (1 4%) 7.2135\n",
      "731m 8s (- 13891m 46s) (1 5%) 6.9713\n",
      "731m 36s (- 13900m 41s) (1 6%) 6.8048\n",
      "732m 4s (- 13909m 34s) (1 7%) 6.9077\n",
      "732m 33s (- 13918m 32s) (1 8%) 6.5687\n",
      "733m 1s (- 13927m 27s) (1 9%) 6.5998\n",
      "733m 29s (- 13936m 21s) (1 10%) 6.7179\n",
      "733m 57s (- 13945m 16s) (1 11%) 6.5389\n",
      "734m 26s (- 13954m 20s) (1 12%) 6.5381\n",
      "734m 54s (- 13963m 11s) (1 13%) 6.4215\n",
      "735m 22s (- 13972m 3s) (1 14%) 6.3214\n",
      "735m 53s (- 13981m 55s) (1 16%) 6.2851\n",
      "736m 26s (- 13992m 28s) (1 17%) 6.3068\n",
      "736m 55s (- 14001m 29s) (1 18%) 6.1833\n",
      "737m 23s (- 14010m 22s) (1 19%) 6.1971\n",
      "737m 50s (- 14019m 8s) (1 20%) 6.2623\n",
      "738m 19s (- 14028m 7s) (1 21%) 6.3628\n",
      "738m 50s (- 14038m 0s) (1 22%) 6.2131\n",
      "739m 18s (- 14046m 49s) (1 23%) 6.1008\n",
      "739m 46s (- 14055m 50s) (1 24%) 6.1008\n",
      "740m 17s (- 14065m 24s) (1 25%) 6.0124\n",
      "740m 45s (- 14074m 25s) (1 26%) 6.7558\n",
      "741m 13s (- 14083m 14s) (1 27%) 6.9054\n",
      "741m 41s (- 14092m 0s) (1 28%) 6.3994\n",
      "742m 9s (- 14100m 59s) (1 29%) 6.3040\n",
      "742m 37s (- 14109m 55s) (1 30%) 6.1636\n",
      "743m 5s (- 14118m 47s) (1 32%) 6.4007\n",
      "743m 33s (- 14127m 39s) (1 33%) 6.3308\n",
      "744m 1s (- 14136m 28s) (1 34%) 6.2402\n",
      "744m 29s (- 14145m 23s) (1 35%) 6.0905\n",
      "744m 57s (- 14154m 20s) (1 36%) 6.0186\n",
      "745m 25s (- 14163m 4s) (1 37%) 5.8815\n",
      "745m 53s (- 14171m 58s) (1 38%) 5.9157\n",
      "746m 21s (- 14180m 56s) (1 39%) 5.7587\n",
      "746m 49s (- 14189m 49s) (1 40%) 5.9045\n",
      "747m 17s (- 14198m 37s) (1 41%) 5.7827\n",
      "747m 45s (- 14207m 26s) (1 42%) 5.8194\n",
      "748m 13s (- 14216m 21s) (1 43%) 5.7008\n",
      "748m 41s (- 14225m 13s) (1 44%) 5.8346\n",
      "749m 9s (- 14234m 9s) (1 45%) 6.0053\n",
      "749m 38s (- 14243m 3s) (1 46%) 6.6788\n",
      "750m 6s (- 14251m 59s) (1 48%) 6.2030\n",
      "750m 34s (- 14260m 54s) (1 49%) 6.1205\n",
      "751m 2s (- 14269m 46s) (1 50%) 6.0894\n",
      "751m 30s (- 14278m 48s) (1 51%) 6.0100\n",
      "751m 59s (- 14287m 42s) (1 52%) 5.9400\n",
      "752m 28s (- 14297m 2s) (1 53%) 5.8627\n",
      "752m 56s (- 14306m 2s) (1 54%) 6.0161\n",
      "753m 25s (- 14314m 57s) (1 55%) 6.0288\n",
      "753m 53s (- 14323m 52s) (1 56%) 6.0206\n",
      "754m 21s (- 14332m 54s) (1 57%) 5.9118\n",
      "754m 49s (- 14341m 41s) (1 58%) 5.8405\n",
      "755m 17s (- 14350m 38s) (1 59%) 6.0546\n",
      "755m 46s (- 14359m 34s) (1 60%) 5.6796\n",
      "756m 14s (- 14368m 30s) (1 61%) 5.8396\n",
      "756m 42s (- 14377m 22s) (1 62%) 5.8910\n",
      "757m 9s (- 14386m 7s) (1 64%) 5.7623\n",
      "757m 38s (- 14395m 3s) (1 65%) 6.3584\n",
      "758m 5s (- 14403m 47s) (1 66%) 6.0221\n",
      "758m 34s (- 14412m 46s) (1 67%) 5.9744\n",
      "759m 1s (- 14421m 35s) (1 68%) 5.8375\n",
      "759m 29s (- 14430m 21s) (1 69%) 5.7606\n",
      "759m 57s (- 14439m 14s) (1 70%) 5.7053\n",
      "760m 26s (- 14448m 17s) (1 71%) 5.7153\n",
      "760m 54s (- 14457m 12s) (1 72%) 5.6859\n",
      "761m 22s (- 14466m 8s) (1 73%) 5.7363\n",
      "761m 50s (- 14475m 1s) (1 74%) 5.8378\n",
      "762m 18s (- 14483m 55s) (1 75%) 5.8934\n",
      "762m 47s (- 14492m 53s) (1 76%) 6.0918\n",
      "763m 15s (- 14501m 45s) (1 77%) 5.8055\n",
      "763m 42s (- 14510m 36s) (1 78%) 5.7237\n",
      "764m 10s (- 14519m 27s) (1 80%) 5.6996\n",
      "764m 38s (- 14528m 19s) (1 81%) 5.7015\n",
      "765m 6s (- 14537m 8s) (1 82%) 5.6405\n",
      "765m 34s (- 14545m 58s) (1 83%) 5.6105\n",
      "766m 2s (- 14554m 43s) (1 84%) 5.6383\n",
      "766m 29s (- 14563m 24s) (1 85%) 5.5655\n",
      "766m 57s (- 14572m 6s) (1 86%) 5.7596\n",
      "767m 24s (- 14580m 52s) (1 87%) 5.5005\n",
      "767m 52s (- 14589m 33s) (1 88%) 5.6194\n",
      "768m 22s (- 14599m 6s) (1 89%) 5.7656\n",
      "768m 53s (- 14608m 58s) (1 90%) 5.6101\n",
      "769m 21s (- 14617m 45s) (1 91%) 5.8081\n",
      "769m 49s (- 14626m 35s) (1 92%) 5.6496\n",
      "770m 17s (- 14635m 26s) (1 93%) 5.6544\n",
      "770m 45s (- 14644m 17s) (1 94%) 5.6316\n",
      "771m 12s (- 14652m 59s) (1 96%) 5.5051\n",
      "771m 40s (- 14661m 44s) (1 97%) 5.7113\n",
      "772m 8s (- 14670m 38s) (1 98%) 5.5681\n",
      "772m 36s (- 14679m 40s) (1 99%) 5.7140\n",
      "val_loss: 6.8721 - bleu: 0\n",
      "781m 13s (- 7031m 3s) (2 1%) 264.8967\n",
      "781m 42s (- 7035m 20s) (2 2%) 5.7029\n",
      "782m 10s (- 7039m 32s) (2 3%) 5.6266\n",
      "782m 39s (- 7043m 56s) (2 4%) 5.6912\n",
      "783m 7s (- 7048m 7s) (2 5%) 5.6191\n",
      "783m 35s (- 7052m 19s) (2 6%) 5.6700\n",
      "784m 3s (- 7056m 27s) (2 7%) 5.8874\n",
      "784m 30s (- 7060m 34s) (2 8%) 5.6147\n",
      "784m 58s (- 7064m 45s) (2 9%) 5.6213\n",
      "785m 26s (- 7068m 55s) (2 10%) 5.5975\n",
      "785m 56s (- 7073m 32s) (2 11%) 5.7218\n",
      "786m 30s (- 7078m 31s) (2 12%) 5.8251\n",
      "786m 59s (- 7082m 53s) (2 13%) 5.5923\n",
      "787m 28s (- 7087m 19s) (2 14%) 5.5917\n",
      "788m 4s (- 7092m 36s) (2 16%) 5.6496\n",
      "788m 32s (- 7096m 52s) (2 17%) 5.6613\n",
      "789m 1s (- 7101m 10s) (2 18%) 5.4450\n",
      "789m 28s (- 7105m 20s) (2 19%) 5.4049\n",
      "789m 57s (- 7109m 33s) (2 20%) 5.4739\n",
      "790m 24s (- 7113m 41s) (2 21%) 5.4863\n",
      "790m 51s (- 7117m 47s) (2 22%) 5.8021\n",
      "791m 19s (- 7121m 54s) (2 23%) 5.6852\n",
      "791m 46s (- 7126m 2s) (2 24%) 5.5101\n",
      "792m 14s (- 7130m 14s) (2 25%) 5.6132\n",
      "792m 43s (- 7134m 27s) (2 26%) 5.5988\n",
      "793m 10s (- 7138m 38s) (2 27%) 5.5642\n",
      "793m 38s (- 7142m 49s) (2 28%) 5.6031\n",
      "794m 6s (- 7146m 58s) (2 29%) 5.6190\n",
      "794m 33s (- 7151m 3s) (2 30%) 5.5029\n",
      "795m 1s (- 7155m 15s) (2 32%) 5.4723\n",
      "795m 28s (- 7159m 20s) (2 33%) 5.4518\n",
      "795m 56s (- 7163m 27s) (2 34%) 5.4996\n",
      "796m 23s (- 7167m 35s) (2 35%) 5.4612\n",
      "796m 51s (- 7171m 43s) (2 36%) 5.3981\n",
      "797m 19s (- 7175m 52s) (2 37%) 5.6043\n",
      "797m 47s (- 7180m 9s) (2 38%) 5.6211\n",
      "798m 15s (- 7184m 21s) (2 39%) 5.7876\n",
      "798m 44s (- 7188m 38s) (2 40%) 5.5918\n",
      "799m 12s (- 7192m 51s) (2 41%) 5.7999\n",
      "799m 40s (- 7197m 5s) (2 42%) 5.5564\n",
      "800m 9s (- 7201m 21s) (2 43%) 5.5583\n",
      "800m 37s (- 7205m 33s) (2 44%) 5.6889\n",
      "801m 9s (- 7210m 26s) (2 45%) 5.6954\n",
      "801m 41s (- 7215m 12s) (2 46%) 5.7009\n",
      "802m 9s (- 7219m 25s) (2 48%) 5.5859\n",
      "802m 37s (- 7223m 37s) (2 49%) 5.4266\n",
      "803m 6s (- 7227m 59s) (2 50%) 5.5207\n",
      "803m 37s (- 7232m 38s) (2 51%) 5.6296\n",
      "804m 5s (- 7236m 47s) (2 52%) 5.3811\n",
      "804m 33s (- 7240m 58s) (2 53%) 5.3106\n",
      "805m 0s (- 7245m 8s) (2 54%) 5.4562\n",
      "805m 29s (- 7249m 23s) (2 55%) 5.4411\n",
      "805m 57s (- 7253m 33s) (2 56%) 5.4678\n",
      "806m 24s (- 7257m 42s) (2 57%) 5.4552\n",
      "806m 52s (- 7261m 54s) (2 58%) 5.3949\n",
      "807m 20s (- 7266m 5s) (2 59%) 5.3271\n",
      "807m 48s (- 7270m 18s) (2 60%) 5.3460\n",
      "808m 16s (- 7274m 31s) (2 61%) 5.3650\n",
      "808m 44s (- 7278m 42s) (2 62%) 5.5894\n",
      "809m 12s (- 7282m 52s) (2 64%) 5.4523\n",
      "809m 40s (- 7287m 4s) (2 65%) 5.4964\n",
      "810m 10s (- 7291m 35s) (2 66%) 5.3766\n",
      "810m 46s (- 7296m 58s) (2 67%) 5.3235\n",
      "811m 14s (- 7301m 11s) (2 68%) 5.4018\n",
      "811m 44s (- 7305m 39s) (2 69%) 5.3209\n",
      "812m 19s (- 7310m 51s) (2 70%) 5.4297\n",
      "812m 47s (- 7315m 9s) (2 71%) 5.4593\n",
      "813m 18s (- 7319m 49s) (2 72%) 5.4533\n",
      "813m 47s (- 7324m 3s) (2 73%) 5.3894\n",
      "814m 14s (- 7328m 14s) (2 74%) 5.3528\n",
      "814m 42s (- 7332m 25s) (2 75%) 5.3906\n",
      "815m 10s (- 7336m 35s) (2 76%) 5.3800\n",
      "815m 38s (- 7340m 46s) (2 77%) 5.4095\n",
      "816m 6s (- 7344m 58s) (2 78%) 5.4621\n",
      "816m 34s (- 7349m 9s) (2 80%) 5.3602\n",
      "817m 2s (- 7353m 20s) (2 81%) 5.4350\n",
      "817m 30s (- 7357m 33s) (2 82%) 5.6386\n",
      "817m 58s (- 7361m 44s) (2 83%) 5.4960\n",
      "818m 26s (- 7365m 58s) (2 84%) 5.5450\n",
      "818m 54s (- 7370m 7s) (2 85%) 5.4911\n",
      "819m 22s (- 7374m 20s) (2 86%) 5.6422\n",
      "819m 50s (- 7378m 34s) (2 87%) 5.6345\n",
      "820m 18s (- 7382m 47s) (2 88%) 5.4642\n",
      "820m 46s (- 7386m 58s) (2 89%) 5.3994\n",
      "821m 14s (- 7391m 6s) (2 90%) 5.3970\n",
      "821m 42s (- 7395m 20s) (2 91%) 5.4864\n",
      "822m 10s (- 7399m 35s) (2 92%) 5.3840\n",
      "822m 38s (- 7403m 45s) (2 93%) 5.3355\n",
      "823m 6s (- 7407m 56s) (2 94%) 5.4142\n",
      "823m 34s (- 7412m 13s) (2 96%) 5.3052\n",
      "824m 2s (- 7416m 24s) (2 97%) 5.2610\n",
      "824m 30s (- 7420m 37s) (2 98%) 5.2582\n",
      "824m 59s (- 7424m 52s) (2 99%) 5.2839\n",
      "val_loss: 6.3832 - bleu: 0\n",
      "833m 26s (- 4722m 50s) (3 1%) 246.0590\n",
      "833m 54s (- 4725m 28s) (3 2%) 5.4151\n",
      "834m 22s (- 4728m 8s) (3 3%) 5.3198\n",
      "834m 50s (- 4730m 46s) (3 4%) 5.3397\n",
      "835m 18s (- 4733m 25s) (3 5%) 5.3356\n",
      "835m 46s (- 4736m 3s) (3 6%) 5.2870\n",
      "836m 14s (- 4738m 41s) (3 7%) 5.2523\n",
      "836m 42s (- 4741m 19s) (3 8%) 5.4438\n",
      "837m 10s (- 4743m 56s) (3 9%) 5.2958\n",
      "837m 37s (- 4746m 35s) (3 10%) 5.2795\n",
      "838m 5s (- 4749m 12s) (3 11%) 5.3113\n",
      "838m 33s (- 4751m 49s) (3 12%) 5.2791\n",
      "839m 1s (- 4754m 28s) (3 13%) 5.2163\n",
      "839m 29s (- 4757m 6s) (3 14%) 5.3382\n",
      "839m 57s (- 4759m 45s) (3 16%) 5.2730\n",
      "840m 25s (- 4762m 22s) (3 17%) 5.2104\n",
      "840m 53s (- 4765m 0s) (3 18%) 5.3512\n",
      "841m 20s (- 4767m 38s) (3 19%) 5.4934\n",
      "841m 48s (- 4770m 15s) (3 20%) 5.3027\n",
      "842m 16s (- 4772m 51s) (3 21%) 5.2381\n",
      "842m 44s (- 4775m 31s) (3 22%) 5.4197\n",
      "843m 12s (- 4778m 10s) (3 23%) 5.4296\n",
      "843m 40s (- 4780m 48s) (3 24%) 5.3540\n",
      "844m 8s (- 4783m 28s) (3 25%) 5.4412\n",
      "844m 36s (- 4786m 9s) (3 26%) 5.3441\n",
      "845m 5s (- 4788m 48s) (3 27%) 5.3132\n",
      "845m 33s (- 4791m 28s) (3 28%) 5.2444\n",
      "846m 1s (- 4794m 7s) (3 29%) 5.3866\n",
      "846m 29s (- 4796m 46s) (3 30%) 5.4582\n",
      "846m 57s (- 4799m 24s) (3 32%) 5.3906\n",
      "847m 25s (- 4802m 4s) (3 33%) 5.2750\n",
      "847m 53s (- 4804m 42s) (3 34%) 5.2039\n",
      "848m 21s (- 4807m 19s) (3 35%) 5.2581\n",
      "848m 48s (- 4809m 55s) (3 36%) 5.2236\n",
      "849m 16s (- 4812m 34s) (3 37%) 5.3006\n",
      "849m 44s (- 4815m 12s) (3 38%) 5.2618\n",
      "850m 12s (- 4817m 51s) (3 39%) 5.2880\n",
      "850m 40s (- 4820m 29s) (3 40%) 5.2686\n",
      "851m 8s (- 4823m 6s) (3 41%) 5.3002\n",
      "851m 36s (- 4825m 44s) (3 42%) 5.3481\n",
      "852m 3s (- 4828m 21s) (3 43%) 5.3221\n",
      "852m 31s (- 4831m 0s) (3 44%) 5.2722\n",
      "852m 59s (- 4833m 36s) (3 45%) 5.2076\n",
      "853m 27s (- 4836m 17s) (3 46%) 5.2543\n",
      "853m 56s (- 4838m 57s) (3 48%) 5.1943\n",
      "854m 24s (- 4841m 36s) (3 49%) 5.4817\n",
      "854m 51s (- 4844m 12s) (3 50%) 5.2209\n",
      "855m 19s (- 4846m 50s) (3 51%) 5.2670\n",
      "855m 47s (- 4849m 31s) (3 52%) 5.3068\n",
      "856m 15s (- 4852m 9s) (3 53%) 5.2151\n",
      "856m 43s (- 4854m 47s) (3 54%) 5.2895\n",
      "857m 11s (- 4857m 25s) (3 55%) 5.1653\n",
      "857m 39s (- 4860m 4s) (3 56%) 5.3388\n",
      "858m 7s (- 4862m 42s) (3 57%) 5.4749\n",
      "858m 35s (- 4865m 22s) (3 58%) 5.4771\n",
      "859m 3s (- 4868m 0s) (3 59%) 5.2458\n",
      "859m 31s (- 4870m 36s) (3 60%) 5.2093\n",
      "859m 59s (- 4873m 14s) (3 61%) 5.1115\n",
      "860m 27s (- 4875m 54s) (3 62%) 5.2124\n",
      "860m 55s (- 4878m 32s) (3 64%) 5.2886\n",
      "861m 23s (- 4881m 11s) (3 65%) 5.2596\n",
      "861m 51s (- 4883m 51s) (3 66%) 5.1740\n",
      "862m 19s (- 4886m 30s) (3 67%) 5.1288\n",
      "862m 47s (- 4889m 9s) (3 68%) 5.2500\n",
      "863m 15s (- 4891m 48s) (3 69%) 5.2517\n",
      "863m 43s (- 4894m 25s) (3 70%) 5.4022\n",
      "864m 11s (- 4897m 2s) (3 71%) 5.3992\n",
      "864m 39s (- 4899m 42s) (3 72%) 5.2390\n",
      "865m 7s (- 4902m 21s) (3 73%) 5.2969\n",
      "865m 35s (- 4905m 0s) (3 74%) 5.1609\n",
      "866m 3s (- 4907m 38s) (3 75%) 5.1742\n",
      "866m 31s (- 4910m 15s) (3 76%) 5.2432\n",
      "866m 58s (- 4912m 54s) (3 77%) 5.4092\n",
      "867m 26s (- 4915m 31s) (3 78%) 5.2828\n",
      "867m 54s (- 4918m 9s) (3 80%) 5.2732\n",
      "868m 22s (- 4920m 48s) (3 81%) 5.2313\n",
      "868m 50s (- 4923m 28s) (3 82%) 5.3256\n",
      "869m 18s (- 4926m 7s) (3 83%) 5.2551\n",
      "869m 46s (- 4928m 44s) (3 84%) 5.3195\n",
      "870m 14s (- 4931m 21s) (3 85%) 5.2803\n",
      "870m 42s (- 4933m 59s) (3 86%) 5.1542\n",
      "871m 9s (- 4936m 35s) (3 87%) 5.1597\n",
      "871m 37s (- 4939m 12s) (3 88%) 5.1377\n",
      "872m 5s (- 4941m 51s) (3 89%) 5.1033\n",
      "872m 33s (- 4944m 31s) (3 90%) 5.0965\n",
      "873m 1s (- 4947m 8s) (3 91%) 5.0059\n",
      "873m 29s (- 4949m 47s) (3 92%) 5.0831\n",
      "873m 57s (- 4952m 25s) (3 93%) 5.2471\n",
      "874m 25s (- 4955m 2s) (3 94%) 5.1177\n",
      "874m 53s (- 4957m 41s) (3 96%) 5.0917\n",
      "875m 21s (- 4960m 20s) (3 97%) 5.1992\n",
      "875m 49s (- 4962m 59s) (3 98%) 5.2353\n",
      "876m 16s (- 4965m 35s) (3 99%) 5.1333\n",
      "val_loss: 6.5374 - bleu: 0\n",
      "884m 41s (- 3538m 44s) (4 1%) 251.7450\n",
      "885m 9s (- 3540m 36s) (4 2%) 5.1730\n",
      "885m 37s (- 3542m 29s) (4 3%) 5.1425\n",
      "886m 5s (- 3544m 21s) (4 4%) 5.3080\n",
      "886m 32s (- 3546m 11s) (4 5%) 5.1547\n",
      "887m 0s (- 3548m 3s) (4 6%) 5.1066\n",
      "887m 28s (- 3549m 53s) (4 7%) 5.1587\n",
      "887m 56s (- 3551m 44s) (4 8%) 5.2356\n",
      "888m 24s (- 3553m 37s) (4 9%) 5.2520\n",
      "888m 52s (- 3555m 28s) (4 10%) 5.2133\n",
      "889m 20s (- 3557m 20s) (4 11%) 5.1625\n",
      "889m 47s (- 3559m 11s) (4 12%) 5.0492\n",
      "890m 15s (- 3561m 2s) (4 13%) 5.1533\n",
      "890m 43s (- 3562m 54s) (4 14%) 5.1587\n",
      "891m 11s (- 3564m 46s) (4 16%) 5.1773\n",
      "891m 39s (- 3566m 37s) (4 17%) 5.0485\n",
      "892m 7s (- 3568m 30s) (4 18%) 5.1407\n",
      "892m 35s (- 3570m 22s) (4 19%) 5.0835\n",
      "893m 3s (- 3572m 15s) (4 20%) 5.1448\n",
      "893m 31s (- 3574m 6s) (4 21%) 5.1739\n",
      "893m 59s (- 3575m 58s) (4 22%) 5.1978\n",
      "894m 27s (- 3577m 49s) (4 23%) 5.2176\n",
      "894m 55s (- 3579m 42s) (4 24%) 5.2464\n",
      "895m 23s (- 3581m 33s) (4 25%) 5.3247\n",
      "895m 51s (- 3583m 25s) (4 26%) 5.1836\n",
      "896m 19s (- 3585m 18s) (4 27%) 5.0901\n",
      "896m 47s (- 3587m 10s) (4 28%) 5.2581\n",
      "897m 15s (- 3589m 2s) (4 29%) 5.3665\n",
      "897m 43s (- 3590m 52s) (4 30%) 5.1158\n",
      "898m 11s (- 3592m 46s) (4 32%) 5.2310\n",
      "898m 39s (- 3594m 37s) (4 33%) 5.1266\n",
      "899m 7s (- 3596m 29s) (4 34%) 5.1727\n",
      "899m 35s (- 3598m 20s) (4 35%) 5.1550\n",
      "900m 3s (- 3600m 12s) (4 36%) 5.1520\n",
      "900m 30s (- 3602m 3s) (4 37%) 4.9893\n",
      "900m 58s (- 3603m 54s) (4 38%) 5.0465\n",
      "901m 26s (- 3605m 47s) (4 39%) 4.9956\n",
      "901m 54s (- 3607m 39s) (4 40%) 5.0566\n",
      "902m 22s (- 3609m 30s) (4 41%) 4.9875\n",
      "902m 50s (- 3611m 22s) (4 42%) 5.0950\n",
      "903m 18s (- 3613m 12s) (4 43%) 5.1290\n",
      "903m 45s (- 3615m 3s) (4 44%) 5.0589\n",
      "904m 13s (- 3616m 55s) (4 45%) 5.0482\n",
      "904m 41s (- 3618m 46s) (4 46%) 5.0919\n",
      "905m 9s (- 3620m 37s) (4 48%) 4.9446\n",
      "905m 37s (- 3622m 29s) (4 49%) 5.0607\n",
      "906m 5s (- 3624m 20s) (4 50%) 5.1653\n",
      "906m 32s (- 3626m 11s) (4 51%) 5.1271\n",
      "907m 0s (- 3628m 2s) (4 52%) 5.0268\n",
      "907m 28s (- 3629m 52s) (4 53%) 4.9589\n",
      "907m 56s (- 3631m 44s) (4 54%) 4.9586\n",
      "908m 23s (- 3633m 34s) (4 55%) 5.0574\n",
      "908m 51s (- 3635m 25s) (4 56%) 4.9258\n",
      "909m 19s (- 3637m 17s) (4 57%) 5.1057\n",
      "909m 46s (- 3639m 7s) (4 58%) 4.9839\n",
      "910m 14s (- 3640m 57s) (4 59%) 5.1010\n",
      "910m 41s (- 3642m 47s) (4 60%) 5.1350\n",
      "911m 10s (- 3644m 40s) (4 61%) 5.0168\n",
      "911m 37s (- 3646m 31s) (4 62%) 5.0753\n",
      "912m 5s (- 3648m 22s) (4 64%) 5.0105\n",
      "912m 32s (- 3650m 10s) (4 65%) 5.0607\n",
      "913m 0s (- 3652m 2s) (4 66%) 5.0639\n",
      "913m 27s (- 3653m 50s) (4 67%) 5.0189\n",
      "913m 55s (- 3655m 41s) (4 68%) 5.0077\n",
      "914m 23s (- 3657m 32s) (4 69%) 5.0395\n",
      "914m 50s (- 3659m 21s) (4 70%) 5.0340\n",
      "915m 18s (- 3661m 13s) (4 71%) 5.0770\n",
      "915m 46s (- 3663m 4s) (4 72%) 4.9620\n",
      "916m 14s (- 3664m 56s) (4 73%) 5.0304\n",
      "916m 41s (- 3666m 46s) (4 74%) 5.1195\n",
      "917m 9s (- 3668m 36s) (4 75%) 5.0325\n",
      "917m 36s (- 3670m 26s) (4 76%) 5.0247\n",
      "918m 4s (- 3672m 16s) (4 77%) 5.1268\n",
      "918m 32s (- 3674m 8s) (4 78%) 5.2440\n",
      "918m 59s (- 3675m 59s) (4 80%) 5.0368\n",
      "919m 27s (- 3677m 49s) (4 81%) 5.0421\n",
      "919m 55s (- 3679m 40s) (4 82%) 5.0550\n",
      "920m 23s (- 3681m 32s) (4 83%) 5.0059\n",
      "920m 50s (- 3683m 22s) (4 84%) 5.0372\n",
      "921m 18s (- 3685m 14s) (4 85%) 5.1221\n",
      "921m 46s (- 3687m 4s) (4 86%) 4.9919\n",
      "922m 14s (- 3688m 56s) (4 87%) 5.0941\n",
      "922m 42s (- 3690m 48s) (4 88%) 5.0548\n",
      "923m 9s (- 3692m 38s) (4 89%) 5.1549\n",
      "923m 37s (- 3694m 28s) (4 90%) 5.1535\n",
      "924m 4s (- 3696m 19s) (4 91%) 5.0546\n",
      "924m 32s (- 3698m 9s) (4 92%) 4.9957\n",
      "924m 59s (- 3699m 59s) (4 93%) 5.0173\n",
      "925m 27s (- 3701m 49s) (4 94%) 4.9635\n",
      "925m 55s (- 3703m 41s) (4 96%) 5.0242\n",
      "926m 23s (- 3705m 32s) (4 97%) 4.9043\n",
      "926m 50s (- 3707m 23s) (4 98%) 4.8845\n",
      "927m 18s (- 3709m 15s) (4 99%) 5.0733\n",
      "val_loss: 6.3597 - bleu: 0\n",
      "935m 37s (- 2806m 52s) (5 1%) 244.9689\n",
      "936m 5s (- 2808m 15s) (5 2%) 5.0099\n",
      "936m 33s (- 2809m 39s) (5 3%) 5.0278\n",
      "937m 1s (- 2811m 4s) (5 4%) 5.1169\n",
      "937m 29s (- 2812m 27s) (5 5%) 5.1471\n",
      "937m 57s (- 2813m 52s) (5 6%) 5.0385\n",
      "938m 25s (- 2815m 16s) (5 7%) 5.0774\n",
      "938m 53s (- 2816m 39s) (5 8%) 5.0858\n",
      "939m 20s (- 2818m 2s) (5 9%) 5.0429\n",
      "939m 48s (- 2819m 24s) (5 10%) 4.9375\n",
      "940m 15s (- 2820m 47s) (5 11%) 4.9209\n",
      "940m 43s (- 2822m 10s) (5 12%) 5.0469\n",
      "941m 10s (- 2823m 32s) (5 13%) 5.0651\n",
      "941m 38s (- 2824m 56s) (5 14%) 5.0352\n",
      "942m 6s (- 2826m 19s) (5 16%) 4.9653\n",
      "942m 34s (- 2827m 43s) (5 17%) 4.9490\n",
      "943m 2s (- 2829m 7s) (5 18%) 4.9380\n",
      "943m 30s (- 2830m 30s) (5 19%) 5.0325\n",
      "943m 57s (- 2831m 53s) (5 20%) 5.0227\n",
      "944m 25s (- 2833m 15s) (5 21%) 4.9977\n",
      "944m 52s (- 2834m 38s) (5 22%) 4.9742\n",
      "945m 20s (- 2836m 2s) (5 23%) 4.9728\n",
      "945m 48s (- 2837m 26s) (5 24%) 5.1618\n",
      "946m 16s (- 2838m 48s) (5 25%) 4.9325\n",
      "946m 44s (- 2840m 12s) (5 26%) 4.9741\n",
      "947m 11s (- 2841m 35s) (5 27%) 5.0591\n",
      "947m 39s (- 2842m 58s) (5 28%) 5.0038\n",
      "948m 7s (- 2844m 21s) (5 29%) 4.9068\n",
      "948m 34s (- 2845m 43s) (5 30%) 4.9484\n",
      "949m 2s (- 2847m 7s) (5 32%) 4.9538\n",
      "949m 30s (- 2848m 30s) (5 33%) 4.8697\n",
      "949m 57s (- 2849m 53s) (5 34%) 4.9706\n",
      "950m 25s (- 2851m 16s) (5 35%) 4.9536\n",
      "950m 53s (- 2852m 39s) (5 36%) 4.9080\n",
      "951m 21s (- 2854m 4s) (5 37%) 5.0067\n",
      "951m 56s (- 2855m 49s) (5 38%) 4.9549\n",
      "952m 31s (- 2857m 35s) (5 39%) 5.0882\n",
      "953m 4s (- 2859m 12s) (5 40%) 5.0978\n",
      "953m 32s (- 2860m 36s) (5 41%) 4.9497\n",
      "954m 0s (- 2862m 1s) (5 42%) 4.9712\n",
      "954m 32s (- 2863m 36s) (5 43%) 4.9705\n",
      "955m 0s (- 2865m 0s) (5 44%) 4.9085\n",
      "955m 28s (- 2866m 25s) (5 45%) 5.0369\n",
      "955m 56s (- 2867m 49s) (5 46%) 4.9864\n",
      "956m 28s (- 2869m 26s) (5 48%) 5.2590\n",
      "957m 1s (- 2871m 5s) (5 49%) 5.2468\n",
      "957m 35s (- 2872m 47s) (5 50%) 4.9871\n",
      "958m 9s (- 2874m 27s) (5 51%) 4.8941\n",
      "958m 43s (- 2876m 9s) (5 52%) 5.0273\n",
      "959m 17s (- 2877m 52s) (5 53%) 5.1644\n",
      "959m 51s (- 2879m 33s) (5 54%) 5.0220\n",
      "960m 24s (- 2881m 13s) (5 55%) 5.0018\n",
      "960m 59s (- 2882m 57s) (5 56%) 5.0443\n",
      "961m 33s (- 2884m 39s) (5 57%) 5.0243\n",
      "962m 6s (- 2886m 20s) (5 58%) 4.9362\n",
      "962m 40s (- 2888m 1s) (5 59%) 4.8507\n",
      "963m 14s (- 2889m 42s) (5 60%) 5.0687\n",
      "963m 47s (- 2891m 23s) (5 61%) 4.9781\n",
      "964m 22s (- 2893m 6s) (5 62%) 5.0779\n",
      "964m 55s (- 2894m 46s) (5 64%) 4.9823\n",
      "965m 29s (- 2896m 27s) (5 65%) 5.0784\n",
      "966m 2s (- 2898m 7s) (5 66%) 4.9115\n",
      "966m 36s (- 2899m 48s) (5 67%) 4.8851\n",
      "967m 9s (- 2901m 27s) (5 68%) 5.0017\n",
      "967m 42s (- 2903m 8s) (5 69%) 4.9625\n",
      "968m 16s (- 2904m 49s) (5 70%) 4.9318\n",
      "968m 49s (- 2906m 29s) (5 71%) 4.9201\n",
      "969m 23s (- 2908m 9s) (5 72%) 5.2195\n",
      "969m 56s (- 2909m 50s) (5 73%) 5.1709\n",
      "970m 30s (- 2911m 30s) (5 74%) 5.0250\n",
      "971m 4s (- 2913m 13s) (5 75%) 4.9884\n",
      "971m 38s (- 2914m 56s) (5 76%) 5.0836\n",
      "972m 12s (- 2916m 37s) (5 77%) 5.0185\n",
      "972m 46s (- 2918m 20s) (5 78%) 4.9590\n",
      "973m 20s (- 2920m 2s) (5 80%) 5.0025\n",
      "973m 54s (- 2921m 43s) (5 81%) 5.0972\n",
      "974m 28s (- 2923m 25s) (5 82%) 5.0122\n",
      "975m 2s (- 2925m 8s) (5 83%) 4.9655\n",
      "975m 37s (- 2926m 51s) (5 84%) 4.9696\n",
      "976m 10s (- 2928m 30s) (5 85%) 4.9350\n",
      "976m 43s (- 2930m 10s) (5 86%) 5.0233\n",
      "977m 16s (- 2931m 50s) (5 87%) 4.9570\n",
      "977m 50s (- 2933m 32s) (5 88%) 4.9936\n",
      "978m 24s (- 2935m 14s) (5 89%) 4.9566\n",
      "978m 59s (- 2936m 57s) (5 90%) 5.0151\n",
      "979m 33s (- 2938m 39s) (5 91%) 5.0459\n",
      "980m 6s (- 2940m 20s) (5 92%) 4.9334\n",
      "980m 40s (- 2942m 0s) (5 93%) 4.9112\n",
      "981m 14s (- 2943m 42s) (5 94%) 4.8503\n",
      "981m 48s (- 2945m 24s) (5 96%) 4.8077\n",
      "982m 21s (- 2947m 5s) (5 97%) 4.8972\n",
      "982m 55s (- 2948m 46s) (5 98%) 4.8639\n",
      "983m 29s (- 2950m 27s) (5 99%) 4.8835\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3fe501d1944b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0minput_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_matrix\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3a046a074cc3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_batches, target_batches, input_matrixes, encoder, decoder, gcn, criterion, batch_ix, train)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 47\u001b[0;31m                 decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs) \n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mall_decoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/wsd-v2/model/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, last_context, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# rnn_input: (seq_len=1, batch_size, embedding_size + encoder_hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# last_hidden: (num_layers, batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;31m# rnn_output: (seq_len=1, batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# hidden: same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs): \n",
    "    # Shuffle data\n",
    "    #id_aux = np.random.permutation(np.arange(len(pairs_train)))\n",
    "    #pairs_train = pairs_train[id_aux]\n",
    "    \n",
    "    # Get the batches for this epoch\n",
    "    input_batches, input_matrixes, target_batches = generate_batches(input_lang, output_lang, 1, pairs_train, arr_dep=matrixes_train, USE_CUDA=USE_CUDA)    \n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    gcn.train()\n",
    "        \n",
    "    for batch_ix, (input_var, input_matrix, target_var) in enumerate(zip(input_batches, input_matrixes, target_batches)):\n",
    "        \n",
    "        # Run the train function\n",
    "        input_matrix = np.array(input_matrix[0])\n",
    "        degree = np.array(np.sum(input_matrix, axis=0))\n",
    "        degree = np.matrix(np.diag(degree))\n",
    "        \n",
    "        input_matrix = torch.FloatTensor(np.linalg.inv(degree) * input_matrix)\n",
    "        if USE_CUDA:\n",
    "            input_matrix = input_matrix.cuda()\n",
    "\n",
    "        loss = train(input_var, target_var, input_matrix,\\\n",
    "                 encoder, decoder, gcn, criterion, batch_ix, train=True)\n",
    "        #loss = train_luong(input_var, target_var, input_var.size(1), True)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if batch_ix == 0: continue            \n",
    "\n",
    "        if batch_ix % (print_every * batch_size)  == 0:\n",
    "            print_loss_avg = print_loss_total / (print_every * batch_size)\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, batch_ix / len(input_batches) * 100, print_loss_avg)\n",
    "            train_losses.append(loss)\n",
    "            print(print_summary)\n",
    "    \n",
    "    input_batches, input_matrixes, target_batches = generate_batches(input_lang, output_lang, 1, pairs_test, arr_dep=matrixes_test, USE_CUDA=USE_CUDA)\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    gcn.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print_loss_total = 0\n",
    "        for batch_ix, (input_var, input_matrix, target_var) in enumerate(zip(input_batches, input_matrixes, target_batches)):\n",
    "    \n",
    "            # Run the train function\n",
    "            input_matrix = np.array(input_matrix[0])\n",
    "            degree = np.array(np.sum(input_matrix, axis=0))\n",
    "            degree = np.matrix(np.diag(degree))\n",
    "\n",
    "            input_matrix = torch.FloatTensor(np.linalg.inv(degree) * input_matrix)\n",
    "            if USE_CUDA:*-\n",
    "                +\n",
    "                +<   \n",
    "                ut_matrix = input_matrix.cuda()\n",
    "            loss = train(input_var, target_var, input_matrix,\\\n",
    "                     encoder, decoder, gcn, criterion, batch_ix, train=False)\n",
    "\n",
    "            print_loss_total += loss\n",
    "            torch.cuda.empty_cache()\n",
    "    val_loss = print_loss_total / len(input_batches)\n",
    "    validation_losses.append(val_loss)\n",
    "    # Evaluating Bleu\n",
    "    #evaluator = Evaluator(encoder, decoder, input_lang, output_lang, MAX_LENGTH, True)\n",
    "    #candidates, references = evaluator.get_candidates_and_references(pairs_test, k_beams=1)\n",
    "    #bleu = BLEU(candidates, [references])\n",
    "    #if bleu[0] > best_bleu:\n",
    "    #    best_bleu = bleu[0]\n",
    "    #    torch.save(encoder.state_dict(), f'{DIR_RESULTS}/encoder.pkl')\n",
    "    #    torch.save(decoder.state_dict(), f'{DIR_RESULTS}/decoder.pkl')\n",
    "    #validation_bleu.append(bleu)\n",
    "    print(f'val_loss: {val_loss:.4f} - bleu: {0}')\n",
    "\n",
    "    # Prevent overflow gpu memory\n",
    "    #del evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-188a0ff1e830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 6. 2. 2. 2. 5. 2. 3. 2. 2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[2., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 6., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 2., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 2., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 5., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 2., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 3., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 2., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.array(np.sum(temp, axis=0))\n",
    "print(D)\n",
    "D = np.matrix(np.diag(D))\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.16666667, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.5       , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.2       , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.5       , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.16666667, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.5       , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.2       , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.5       , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(temp, np.linalg.inv(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.16666667, 0.16666667, 0.16666667, 0.        , 0.        ,\n",
       "         0.16666667, 0.        , 0.        , 0.16666667, 0.16666667],\n",
       "        [0.        , 0.5       , 0.5       , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.5       , 0.        ,\n",
       "         0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "         0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.2       , 0.        , 0.2       , 0.2       ,\n",
       "         0.2       , 0.        , 0.2       , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.5       , 0.5       , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.33333333, 0.33333333, 0.33333333, 0.        , 0.        ],\n",
       "        [0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.5       , 0.        ],\n",
       "        [0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.5       ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D**-1*temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 1]), (7, 7))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = 0\n",
    "input_batches[ix].shape, input_matrixes[ix][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he gave me 10 ,000 yen . <eos>'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([input_lang.vocab.itos[w] for w in input_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(encoder, decoder, input_lang, output_lang, \n",
    "                      MAX_LENGTH, USE_CUDA)\n",
    "candidates, references = evaluator.get_candidates_and_references(pairs_test[:10000], k_beams=2)\n",
    "len(candidates), len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28063523097173265,\n",
       " [0.6573135078342698,\n",
       "  0.37567686039915427,\n",
       "  0.22488307382629802,\n",
       "  0.13494545201862276],\n",
       " 0.953820572858132)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLEU(candidates, [references]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.041672706604004,\n",
       " 7.149496555328369,\n",
       " 14.810012817382812,\n",
       " 12.101698875427246,\n",
       " 9.06682014465332,\n",
       " 7.260944366455078,\n",
       " 6.044005393981934,\n",
       " 6.0010833740234375,\n",
       " 7.1673264503479,\n",
       " 6.06639289855957,\n",
       " 6.229933261871338,\n",
       " 8.623738288879395,\n",
       " 5.882908344268799,\n",
       " 4.927737236022949,\n",
       " 5.671040058135986,\n",
       " 7.489520072937012,\n",
       " 5.501565456390381,\n",
       " 5.914140224456787,\n",
       " 6.912267208099365,\n",
       " 6.224165916442871,\n",
       " 7.6937079429626465,\n",
       " 6.377658843994141,\n",
       " 8.010522842407227,\n",
       " 8.193879127502441,\n",
       " 7.284544944763184,\n",
       " 4.864162445068359,\n",
       " 6.447579860687256,\n",
       " 6.805881977081299,\n",
       " 5.03970193862915,\n",
       " 6.144567012786865,\n",
       " 5.522188186645508,\n",
       " 6.3946533203125,\n",
       " 7.317024230957031,\n",
       " 7.084739685058594,\n",
       " 4.866414546966553,\n",
       " 4.8789286613464355,\n",
       " 6.360021114349365,\n",
       " 5.258521556854248,\n",
       " 7.594843864440918,\n",
       " 5.99109411239624,\n",
       " 6.1218085289001465,\n",
       " 4.6263885498046875,\n",
       " 6.505831241607666,\n",
       " 6.49678897857666,\n",
       " 6.661844253540039,\n",
       " 5.703457832336426,\n",
       " 6.080120086669922,\n",
       " 5.556210994720459,\n",
       " 4.3718485832214355,\n",
       " 7.616245269775391,\n",
       " 5.660667896270752,\n",
       " 4.477021217346191,\n",
       " 5.971453666687012,\n",
       " 7.151060581207275,\n",
       " 7.284686088562012,\n",
       " 4.710358619689941,\n",
       " 5.699878692626953,\n",
       " 5.8283185958862305,\n",
       " 5.429839134216309,\n",
       " 5.627746105194092,\n",
       " 8.024958610534668,\n",
       " 7.326678276062012,\n",
       " 6.238666534423828,\n",
       " 4.728693962097168,\n",
       " 6.361328125,\n",
       " 5.92225456237793,\n",
       " 6.291927337646484,\n",
       " 6.011835098266602,\n",
       " 7.155662536621094,\n",
       " 5.337331295013428,\n",
       " 7.359513282775879,\n",
       " 6.708942890167236,\n",
       " 6.291254997253418,\n",
       " 6.5870819091796875,\n",
       " 6.9649858474731445,\n",
       " 5.453511714935303,\n",
       " 5.605909824371338,\n",
       " 6.4293036460876465,\n",
       " 6.750390529632568,\n",
       " 8.476428031921387,\n",
       " 5.792107582092285,\n",
       " 7.52388334274292,\n",
       " 5.88793420791626,\n",
       " 5.64914083480835,\n",
       " 5.583569049835205,\n",
       " 7.05214786529541,\n",
       " 7.007693767547607,\n",
       " 6.556692600250244,\n",
       " 5.914644241333008,\n",
       " 7.384115219116211,\n",
       " 6.446426868438721,\n",
       " 5.6869215965271,\n",
       " 6.317768573760986,\n",
       " 6.537539958953857,\n",
       " 8.257403373718262,\n",
       " 6.223647594451904,\n",
       " 7.760086536407471,\n",
       " 6.339871883392334,\n",
       " 6.288576602935791,\n",
       " 6.117007732391357,\n",
       " 4.761729717254639,\n",
       " 5.098626136779785,\n",
       " 5.2466816902160645,\n",
       " 4.348966121673584,\n",
       " 6.218953609466553,\n",
       " 4.525362968444824,\n",
       " 8.28030014038086,\n",
       " 5.524833679199219,\n",
       " 7.029933452606201,\n",
       " 5.692931175231934,\n",
       " 5.107240200042725,\n",
       " 7.430395126342773,\n",
       " 6.536320686340332,\n",
       " 6.89471960067749,\n",
       " 4.141633987426758,\n",
       " 5.832042217254639,\n",
       " 5.386961460113525,\n",
       " 5.803618907928467,\n",
       " 6.419718265533447,\n",
       " 6.172667503356934,\n",
       " 7.619015216827393,\n",
       " 6.166060447692871,\n",
       " 6.7607011795043945,\n",
       " 5.820564270019531,\n",
       " 6.111392021179199,\n",
       " 4.593202114105225,\n",
       " 6.3956732749938965,\n",
       " 4.5033745765686035,\n",
       " 5.831901550292969,\n",
       " 6.683056354522705,\n",
       " 6.531650543212891,\n",
       " 4.357241630554199,\n",
       " 6.526559829711914,\n",
       " 5.965932369232178,\n",
       " 7.174294948577881,\n",
       " 5.76188325881958,\n",
       " 5.928871154785156,\n",
       " 6.473082542419434,\n",
       " 6.172927379608154,\n",
       " 6.565548896789551]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"tom wasn 't convinced it was a good idea .\",\n",
       "       'tom no estaba convencido de que fuera una buena idea .'],\n",
       "      dtype='<U245')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_test[480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['just act as if nothing has happened .',\n",
       "       'haga de cuenta que nada ha ocurrido .'],\n",
       "      dtype='<U245')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_train[80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
