{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import construct_pairs\n",
    "from Preprocessing import construct_vectors\n",
    "from Preprocessing import prepare_data\n",
    "from Preprocessing import load_senses\n",
    "from Preprocessing import random_batch\n",
    "\n",
    "from Utils import load_json\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 600\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8642, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_train = 'data_train_con_poda.json'\n",
    "pairs_train = load_json(file_path_train)\n",
    "pairs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4252, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_test = 'data_test_con_poda.json'\n",
    "pairs_test = load_json(file_path_test)\n",
    "pairs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pairs 8642\n",
      "Filtered to 8642 pairs\n",
      "Indexing words...\n",
      "Indexed 17051 words in input language, 17844 words in output\n"
     ]
    }
   ],
   "source": [
    "sentence, sense = prepare_data(pairs_train, pairs_test, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['they are activated by the muscles .',\n",
       "       'they are activated_38201 by the muscles .', 'activated_38201', '6'],\n",
       "      dtype='<U376')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_train[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = np.concatenate((pairs_train, pairs_test), axis=0)\n",
    "sent_vect, sens_vect = construct_vectors(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 66 party_party11401\n",
      "44 69 degree_degree10700\n",
      "73 76 arms_arm10800\n",
      "65 80 audience_audience11400\n",
      "52 83 sort_sort10900\n",
      "61 93 atmosphere_atmosphere12601\n",
      "81 97 activated_38201\n",
      "93 100 banks_bank11400\n",
      "63 110 argument_argument11002\n",
      "104 111 arm_arm10800\n",
      "67 115 bank_bank11400\n",
      "98 120 plan_plan10900\n",
      "94 125 appear_190902\n",
      "71 127 interest_interest10900\n",
      "64 128 shelter_shelter12600\n",
      "122 130 eat_1297001\n",
      "139 133 talk_4198501\n",
      "120 134 decided_1067503\n",
      "118 135 hot_hot30001\n",
      "48 137 paper_paper12700\n",
      "64 142 shelter_shelter10600\n",
      "66 149 difference_difference10700\n",
      "52 153 sort_sort10700\n",
      "86 154 added_42603\n",
      "123 157 plans_plan10900\n",
      "130 160 provide_3313902\n",
      "64 164 shelter_shelter10601\n",
      "63 166 argument_argument11000\n",
      "61 167 atmosphere_atmosphere10700\n",
      "59 168 asked_238101\n",
      "84 169 disc_disc10601\n",
      "59 173 asked_238105\n",
      "86 175 added_42601\n",
      "69 176 different_different30000\n",
      "136 177 expected_1440301\n",
      "142 178 noted_2822013\n",
      "92 180 image_image10700\n",
      "119 185 add_42601\n",
      "153 187 expressed_1446801\n",
      "44 191 degree_degree11000\n",
      "156 194 appears_190902\n",
      "66 195 difference_difference12400\n",
      "218 196 organizations_organization11400\n",
      "125 197 encountered_1353101\n",
      "148 199 produce_3288301\n",
      "70 213 performance_performance10400\n",
      "111 214 suspended_4155305\n",
      "65 216 audience_audience11401\n",
      "92 225 image_image10600\n",
      "174 237 activate_38201\n",
      "128 243 arguments_argument11002\n",
      "182 249 remains_3477801\n",
      "204 252 eating_1297001\n",
      "125 253 encountered_1353103\n",
      "152 254 expect_1440301\n",
      "131 261 differences_difference10700\n",
      "70 264 performance_performance10401\n",
      "162 267 organization_organization11400\n",
      "48 268 paper_paper11003\n",
      "205 274 remain_3477801\n",
      "124 275 began_369201\n",
      "84 276 disc_disc10600\n",
      "115 285 appeared_190903\n",
      "44 291 degree_degree12601\n",
      "63 294 argument_argument11003\n",
      "187 297 mean_2555502\n",
      "48 309 paper_paper11000\n",
      "70 315 performance_performance12200\n",
      "250 317 remained_3477801\n",
      "66 322 difference_difference12300\n",
      "94 332 appear_190901\n",
      "115 333 appeared_190901\n",
      "84 335 disc_disc10603\n",
      "181 337 interests_interest10701\n",
      "42 340 party_party11100\n",
      "70 341 performance_performance11000\n",
      "67 344 bank_bank11701\n",
      "176 348 smell_3893501\n",
      "71 352 interest_interest10702\n",
      "48 354 paper_paper11002\n",
      "278 358 watched_4640507\n",
      "347 361 eaten_1297001\n",
      "152 362 expect_1440302\n",
      "111 370 suspended_4155302\n",
      "124 374 began_369203\n",
      "141 381 produced_3288301\n",
      "134 391 discs_disc10600\n",
      "134 392 discs_disc10601\n",
      "59 402 asked_u\n",
      "142 408 noted_2822011\n",
      "141 411 produced_3288302\n",
      "81 417 activated_38203\n",
      "67 418 bank_bank10600\n",
      "198 429 provided_3313902\n",
      "335 430 shelters_shelter10600\n",
      "200 432 source_source11500\n",
      "285 433 watch_4640507\n",
      "311 436 activating_38201\n",
      "73 437 arms_arm10601\n",
      "66 455 difference_difference11100\n",
      "136 456 expected_1440302\n",
      "276 463 talking_4198501\n",
      "129 465 ask_238101\n",
      "131 469 differences_difference11000\n",
      "98 477 plan_plan10901\n",
      "163 483 treated_4380102\n",
      "81 488 activated_38202\n",
      "59 489 asked_238102\n",
      "326 500 received_3434801\n",
      "214 507 audiences_audience11400\n",
      "69 509 different_different30002\n",
      "69 510 different_different50001other00\n",
      "119 519 add_42606\n",
      "115 521 appeared_190902\n",
      "180 524 climbing_770001\n",
      "180 525 climbing_770005\n",
      "84 526 disc_disc12500\n",
      "256 529 ruled_3597910\n",
      "230 537 adding_42601\n",
      "214 538 audiences_audience11401\n",
      "260 541 climbed_770001\n",
      "297 554 note_2822011\n",
      "132 555 papers_paper11003\n",
      "461 580 activates_38201\n",
      "197 582 begin_369201\n",
      "468 593 provides_3313902\n",
      "309 598 washed_4636101\n",
      "129 623 ask_238102\n",
      "129 624 ask_238105\n",
      "134 629 discs_disc10603\n",
      "71 632 interest_interest10701\n",
      "367 638 sorts_sort10900\n",
      "394 642 watching_4640507\n",
      "61 648 atmosphere_atmosphere11500\n",
      "313 654 climb_770001\n",
      "132 666 papers_paper11002\n",
      "256 668 ruled_3597906\n",
      "372 669 smelling_3893505\n",
      "128 677 arguments_argument11000\n",
      "291 687 images_image10600\n",
      "208 689 judgment_judgment10400\n",
      "163 704 treated_4380101\n",
      "260 710 climbed_770005\n",
      "281 713 degrees_degree10700\n",
      "162 724 organization_organization10900\n",
      "48 726 paper_paper11001\n",
      "194 727 parties_party11401\n",
      "148 729 produce_3288302\n",
      "518 731 receive_3434801\n",
      "86 741 added_42606\n",
      "156 743 appears_190901\n",
      "283 762 heard_1892102\n",
      "190 764 lost_2439904\n",
      "320 770 played_3165210\n",
      "174 781 activate_38202\n",
      "617 791 expecting_1440301\n",
      "283 795 heard_1892101\n",
      "140 796 important_important30000\n",
      "140 797 important_important30002\n",
      "48 803 paper_u\n",
      "132 804 papers_paper11000\n",
      "454 807 providing_3313902\n",
      "111 814 suspended_4155301\n",
      "507 815 talked_4198501\n",
      "321 820 adds_42603\n",
      "104 822 arm_arm11400\n",
      "59 823 asked_238103\n",
      "653 824 ate_1297001\n",
      "384 833 express_1446801\n",
      "213 845 playing_3165210\n",
      "141 846 produced_3288305\n",
      "299 847 producing_3288301\n",
      "166 855 solid_solid50000good01\n",
      "52 857 sort_sort11800\n",
      "288 861 win_4711404\n",
      "313 870 climb_770005\n",
      "382 873 decide_1067503\n",
      "120 874 decided_1067501\n",
      "122 878 eat_1297006\n",
      "181 887 interests_interest12103\n",
      "42 894 party_party11400\n",
      "196 902 simple_simple50000easy01\n",
      "176 903 smell_3893505\n",
      "302 910 won_4711404\n",
      "128 912 arguments_argument11003\n",
      "59 913 asked_238106\n",
      "93 915 banks_bank11701\n",
      "272 918 beginning_369201\n",
      "131 926 differences_difference11100\n",
      "153 929 expressed_1446804\n",
      "92 931 image_image10900\n",
      "362 938 lose_2439904\n",
      "48 939 paper_paper11400\n",
      "194 940 parties_party11800\n",
      "42 941 party_party11402\n",
      "605 947 produces_3288301\n",
      "196 951 simple_simple30002\n",
      "163 959 treated_4380107\n",
      "271 968 asking_238101\n",
      "322 971 begun_369201\n",
      "401 984 hear_1892102\n",
      "123 992 plans_plan10600\n",
      "552 1010 wash_4636101\n",
      "94 1016 appear_190903\n",
      "124 1021 began_369202\n",
      "197 1022 begin_369203\n",
      "134 1030 discs_disc12500\n",
      "448 1032 encounter_1353101\n",
      "140 1042 important_important30004\n",
      "70 1049 performance_performance10403\n",
      "98 1052 plan_plan10600\n",
      "299 1054 producing_3288302\n",
      "357 1065 treat_4380101\n",
      "119 1073 add_42603\n",
      "230 1074 adding_42603\n",
      "129 1079 ask_238106\n",
      "197 1080 begin_u\n",
      "63 1147 argument_argument11001\n",
      "65 1148 audience_audience12600\n",
      "382 1158 decide_1067501\n",
      "306 1160 difficulty_difficulty10400\n",
      "401 1174 hear_1892101\n",
      "319 1187 means_2555505\n",
      "316 1206 sources_source11500\n",
      "288 1218 win_4711401\n",
      "352 1223 write_4753408\n",
      "271 1229 asking_238105\n",
      "44 1247 degree_degree10701\n",
      "448 1251 encounter_1353103\n",
      "71 1269 interest_interest12100\n",
      "208 1272 judgment_judgment10904\n",
      "187 1274 mean_2555505\n",
      "341 1277 missed_2644303\n",
      "194 1284 parties_party11100\n",
      "365 1286 performances_performance10401\n",
      "609 1300 smelled_3893505\n",
      "61 1318 atmosphere_u\n",
      "281 1330 degrees_degree11000\n",
      "305 1332 difficulties_difficulty12600\n",
      "726 1350 judgments_judgment10402\n",
      "341 1359 missed_2644304\n",
      "868 1370 operated_2893205\n",
      "42 1372 party_u\n",
      "365 1373 performances_performance11000\n",
      "198 1379 provided_3313903\n",
      "205 1384 remain_3477802\n",
      "166 1396 solid_solid50000sound01\n",
      "740 1404 suspend_4155302\n",
      "291 1452 images_image10900\n",
      "761 1471 noting_2822013\n",
      "132 1476 papers_paper11001\n",
      "123 1479 plans_plan10901\n",
      "111 1503 suspended_4155304\n",
      "276 1504 talking_4198506\n",
      "321 1521 adds_42601\n",
      "321 1522 adds_42606\n",
      "104 1527 arm_arm10603\n",
      "67 1531 bank_bank11700\n",
      "66 1551 difference_difference11000\n",
      "69 1552 different_different50000unusual00\n",
      "305 1553 difficulties_difficulty10902\n",
      "204 1561 eating_1297006\n",
      "943 1568 expects_1440301\n",
      "208 1581 judgment_judgment10402\n",
      "667 1599 operate_2893202\n",
      "162 1602 organization_organization11401\n",
      "221 1608 play_3165210\n",
      "168 1639 used_4530705\n",
      "302 1643 won_4711401\n",
      "311 1648 activating_38202\n",
      "272 1661 beginning_369203\n",
      "591 1662 begins_369201\n",
      "306 1681 difficulty_difficulty10902\n",
      "190 1715 lost_2439901\n",
      "221 1737 play_3165211\n",
      "196 1767 simple_simple50000plain01\n",
      "1094 1770 smells_3893505\n",
      "357 1784 treat_4380102\n",
      "278 1795 watched_4640501\n",
      "129 1807 ask_238103\n",
      "59 1808 asked_238104\n",
      "272 1815 beginning_369202\n",
      "591 1816 begins_369202\n",
      "898 1843 deciding_1067503\n",
      "384 1857 express_1446804\n",
      "501 1896 miss_2644306\n",
      "213 1909 playing_3165217\n",
      "176 1935 smell_3893503\n",
      "372 1936 smelling_3893501\n",
      "166 1939 solid_solid50000hard01\n",
      "677 1946 talks_4198501\n",
      "163 1951 treated_4380105\n",
      "168 1955 used_4530701\n",
      "230 1973 adding_42606\n",
      "63 1984 argument_argument10900\n",
      "128 1985 arguments_argument10900\n",
      "271 1988 asking_238106\n",
      "61 1991 atmosphere_atmosphere11700\n",
      "93 1995 banks_bank11401\n",
      "322 1996 begun_u\n",
      "306 2036 difficulty_difficulty12600\n",
      "341 2106 missed_2644302\n",
      "297 2117 note_2822013\n",
      "142 2118 noted_2822012\n",
      "761 2119 noting_2822011\n",
      "221 2133 play_3165217\n",
      "961 2147 remaining_3477801\n",
      "211 2187 use_4530701\n",
      "73 2212 arms_u\n",
      "61 2213 atmosphere_atmosphere12600\n",
      "67 2218 bank_bank10400\n",
      "197 2220 begin_369202\n",
      "180 2246 climbing_770003\n",
      "281 2269 degrees_degree12601\n",
      "122 2287 eat_1297003\n",
      "118 2328 hot_u\n",
      "140 2331 important_important50000influential00\n",
      "562 2355 meant_2555504\n",
      "48 2369 paper_paper10600\n",
      "132 2370 papers_u\n",
      "182 2400 remains_3477802\n",
      "166 2418 solid_solid50000homogeneous00\n",
      "166 2419 solid_solid50000unbroken02\n",
      "200 2422 source_source11001\n",
      "200 2423 source_source11800\n",
      "316 2424 sources_source11001\n",
      "802 2453 washing_4636101\n",
      "285 2454 watch_4640504\n",
      "302 2462 won_4711403\n",
      "174 2474 activate_38203\n",
      "81 2475 activated_u\n",
      "119 2478 add_42605\n",
      "63 2493 argument_u\n",
      "128 2494 arguments_argument11001\n",
      "129 2498 ask_u\n",
      "180 2538 climbing_770002\n",
      "69 2563 different_different50000other00\n",
      "1405 2571 encounters_1353103\n",
      "384 2583 express_1446802\n",
      "153 2584 expressed_1446802\n",
      "944 2585 expressing_1446801\n",
      "118 2613 hot_hot30002\n",
      "118 2614 hot_hot50000pungent00\n",
      "92 2619 image_u\n",
      "181 2624 interests_interest10401\n",
      "362 2633 lose_2439903\n",
      "190 2635 lost_2439902\n",
      "501 2650 miss_2644303\n",
      "365 2674 performances_performance10400\n",
      "839 2718 rule_3597910\n",
      "196 2731 simple_simple50002plain01\n",
      "609 2735 smelled_3893508\n",
      "166 2736 solid_solid30002\n",
      "166 2737 solid_solid50000honorable00\n",
      "970 2766 treating_4380102\n",
      "288 2780 win_4711403\n",
      "803 2782 winning_4711404\n",
      "352 2786 write_4753402\n",
      "156 2812 appears_190903\n",
      "271 2815 asking_238102\n",
      "65 2820 audience_u\n",
      "93 2825 banks_bank11700\n",
      "124 2827 began_u\n",
      "322 2828 begun_369202\n",
      "322 2829 begun_369203\n",
      "44 2891 degree_degree12303\n",
      "306 2901 difficulty_difficulty10700\n",
      "1843 2933 expresses_1446801\n",
      "208 3007 judgment_judgment10900\n",
      "829 3021 losing_2439904\n",
      "190 3022 lost_2439903\n",
      "341 3035 missed_2644306\n",
      "867 3045 notes_2822013\n",
      "667 3050 operate_2893205\n",
      "132 3060 papers_paper11400\n",
      "132 3061 papers_paper12700\n",
      "194 3063 parties_u\n",
      "42 3064 party_party11800\n",
      "70 3071 performance_u\n",
      "213 3083 playing_3165213\n",
      "605 3099 produces_3288302\n",
      "130 3105 provide_3313901\n",
      "839 3147 rule_3597906\n",
      "335 3169 shelters_shelter10601\n",
      "166 3175 solid_u\n",
      "367 3177 sorts_sort11800\n",
      "200 3178 source_source10900\n",
      "316 3179 sources_source11000\n",
      "970 3210 treating_4380101\n",
      "309 3228 washed_4636110\n",
      "802 3229 washing_4636107\n",
      "1973 3235 wins_4711404\n",
      "375 3240 writing_4753408\n",
      "119 3264 add_42604\n",
      "86 3265 added_42604\n",
      "128 3284 arguments_u\n",
      "129 3288 ask_238104\n",
      "67 3298 bank_bank11401\n",
      "591 3303 begins_369203\n",
      "1197 3340 climbs_u\n",
      "3047 3380 decides_1067503\n",
      "66 3399 difference_u\n",
      "131 3400 differences_difference12400\n",
      "448 3423 encounter_1353104\n",
      "125 3424 encountered_1353104\n",
      "1836 3425 encountering_1353103\n",
      "943 3437 expects_1440302\n",
      "944 3439 expressing_1446802\n",
      "944 3440 expressing_1446804\n",
      "92 3506 image_image10902\n",
      "71 3515 interest_interest10401\n",
      "208 3530 judgment_judgment10700\n",
      "208 3531 judgment_judgment11000\n",
      "726 3532 judgments_judgment10904\n",
      "1432 3582 missing_2644303\n",
      "1439 3613 operating_2893202\n",
      "98 3638 plan_u\n",
      "123 3639 plans_u\n",
      "221 3643 play_3165219\n",
      "320 3644 played_3165213\n",
      "213 3645 playing_3165211\n",
      "148 3664 produce_3288306\n",
      "130 3671 provide_3313905\n",
      "198 3672 provided_3313901\n",
      "182 3699 remains_3477803\n",
      "736 3723 rules_3597906\n",
      "166 3756 solid_solid50000undiversified00\n",
      "111 3784 suspended_4155306\n",
      "1467 3785 suspending_4155302\n",
      "139 3792 talk_4198506\n",
      "507 3793 talked_4198502\n",
      "357 3816 treat_4380104\n",
      "357 3817 treat_4380105\n",
      "531 3833 using_4530701\n",
      "552 3850 wash_4636107\n",
      "394 3853 watching_4640501\n",
      "803 3863 winning_4711401\n",
      "352 3868 write_4753403\n",
      "375 3869 writing_4753402\n",
      "358 3870 written_4753402\n",
      "358 3871 written_4753403\n",
      "461 3899 activates_38202\n",
      "311 3900 activating_38203\n",
      "321 3902 adds_42605\n",
      "1620 3925 appearing_190903\n",
      "104 3932 arm_arm10602\n",
      "73 3933 arms_arm11400\n",
      "1278 3936 asks_238101\n",
      "1278 3937 asks_238102\n",
      "67 3954 bank_u\n",
      "93 3956 banks_bank10600\n",
      "1197 4028 climbs_770001\n",
      "1197 4029 climbs_770005\n",
      "281 4110 degrees_degree12300\n",
      "131 4124 differences_difference12300\n",
      "69 4125 different_u\n",
      "305 4126 difficulties_difficulty10400\n",
      "134 4131 discs_u\n",
      "347 4162 eaten_1297006\n",
      "283 4275 heard_1892106\n",
      "118 4292 hot_hot50000popular00\n",
      "291 4298 images_u\n",
      "71 4325 interest_interest12103\n",
      "181 4326 interests_interest10900\n",
      "726 4341 judgments_judgment10400\n",
      "362 4366 lose_2439902\n",
      "1427 4367 loses_2439904\n",
      "829 4368 losing_2439903\n",
      "190 4369 lost_2439906\n",
      "319 4383 means_2555502\n",
      "319 4384 means_2555503\n",
      "562 4385 meant_2555502\n",
      "562 4386 meant_2555505\n",
      "501 4406 miss_2644302\n",
      "667 4455 operate_2893201\n",
      "1439 4456 operating_2893201\n",
      "365 4481 performances_performance10403\n",
      "221 4500 play_3165214\n",
      "221 4501 play_3165218\n",
      "320 4502 played_3165211\n",
      "320 4503 played_3165214\n",
      "320 4504 played_3165218\n",
      "213 4505 playing_3165214\n",
      "213 4506 playing_3165218\n",
      "213 4507 playing_3165220\n",
      "835 4508 plays_3165210\n",
      "835 4509 plays_3165211\n",
      "148 4531 produce_3288305\n",
      "198 4545 provided_3313905\n",
      "1924 4561 receiving_3434801\n",
      "250 4572 remained_3477802\n",
      "176 4654 smell_3893508\n",
      "200 4661 source_source10600\n",
      "316 4662 sources_source10600\n",
      "740 4708 suspend_4155303\n",
      "111 4709 suspended_4155303\n",
      "1467 4710 suspending_4155304\n",
      "139 4720 talk_4198503\n",
      "163 4746 treated_4380103\n",
      "309 4793 washed_4636107\n",
      "285 4794 watch_4640501\n",
      "394 4795 watching_4640509\n",
      "302 4810 won_u\n",
      "358 4814 written_4753407\n",
      "473 4815 wrote_4753402\n",
      "473 4816 wrote_4753403\n",
      "86 4874 added_u\n",
      "115 4913 appeared_u\n",
      "1620 4914 appearing_190901\n",
      "1620 4915 appearing_190902\n",
      "104 4924 arm_u\n",
      "73 4925 arms_arm10603\n",
      "271 4929 asking_238103\n",
      "653 4933 ate_1297006\n",
      "67 4945 bank_bank11702\n",
      "67 4946 bank_bank12100\n",
      "124 4956 began_369204\n",
      "272 4957 beginning_u\n",
      "382 5172 decide_1067504\n",
      "120 5173 decided_1067504\n",
      "898 5175 deciding_1067501\n",
      "281 5187 degrees_degree12303\n",
      "84 5219 disc_u\n",
      "1836 5268 encountering_1353104\n",
      "1405 5269 encounters_1353101\n",
      "92 5452 image_image11000\n",
      "181 5485 interests_interest11400\n",
      "190 5575 lost_2439905\n",
      "187 5611 mean_2555503\n",
      "868 5689 operated_2893202\n",
      "218 5696 organizations_organization11401\n",
      "48 5710 paper_paper12700s\n",
      "194 5715 parties_party11402\n",
      "320 5766 played_3165217\n",
      "213 5767 playing_3165219\n",
      "141 5809 produced_3288306\n",
      "326 5861 received_3434802\n",
      "736 5929 rules_3597911\n",
      "64 5974 shelter_u\n",
      "335 5975 shelters_shelter12600\n",
      "176 6009 smell_3893507\n",
      "176 6010 smell_u\n",
      "609 6011 smelled_3893501\n",
      "372 6013 smelling_3893508\n",
      "5524 6014 smelt_3893501\n",
      "166 6021 solid_solid30001\n",
      "166 6022 solid_solid50000plain02\n",
      "52 6026 sort_sort12200\n",
      "52 6027 sort_u\n",
      "111 6097 suspended_4155307\n",
      "139 6109 talk_4198508\n",
      "677 6110 talks_4198506\n",
      "357 6163 treat_4380106\n",
      "163 6164 treated_4380106\n",
      "163 6165 treated_4380108\n",
      "970 6166 treating_4380107\n",
      "2197 6167 treats_4380101\n",
      "309 6245 washed_4636103\n",
      "309 6246 washed_4636104\n",
      "1973 6261 wins_4711401\n",
      "81 6393 activated_38204\n",
      "461 6394 activates_38203\n",
      "94 6492 appear_u\n",
      "1278 6537 asks_238105\n",
      "65 6562 audience_audience11000\n",
      "214 6563 audiences_u\n",
      "93 6593 banks_bank11702\n",
      "197 6621 begin_369204\n",
      "272 6622 beginning_369204\n",
      "313 6857 climb_770004\n",
      "260 6858 climbed_770002\n",
      "260 6859 climbed_770004\n",
      "898 7074 deciding_1067504\n",
      "281 7090 degrees_degree10701\n",
      "204 7225 eating_1297003\n",
      "125 7272 encountered_1353102\n",
      "617 7335 expecting_1440302\n",
      "617 7336 expecting_1440303\n",
      "1843 7344 expresses_1446804\n",
      "6892 7422 forearm_arm10800\n",
      "283 7592 heard_1892104\n",
      "118 7639 hot_hot50000active01\n",
      "118 7640 hot_hot50000fresh01\n",
      "92 7663 image_image10601\n",
      "291 7664 images_image10700\n",
      "71 7737 interest_u\n",
      "208 7795 judgment_judgment10901\n",
      "362 7922 lose_2439905\n",
      "829 7923 losing_2439902\n",
      "829 7924 losing_2439907\n",
      "187 7998 mean_2555501\n",
      "187 7999 mean_2555506\n",
      "501 8050 miss_2644304\n",
      "341 8051 missed_2644301\n",
      "2735 8052 misses_2644301\n",
      "2735 8053 misses_2644302\n",
      "2735 8054 misses_2644306\n",
      "1432 8055 missing_2644301\n",
      "1432 8056 missing_2644302\n",
      "297 8172 note_2822012\n",
      "4039 8214 operates_2893202\n",
      "1439 8215 operating_2893205\n",
      "194 8284 parties_party11400\n",
      "98 8362 plan_plan10901ting\n",
      "213 8369 playing_u\n",
      "141 8465 produced_3288304\n",
      "518 8555 receive_3434805\n",
      "326 8556 received_3434804\n",
      "5380 8557 receives_3434801\n",
      "1924 8559 receiving_3434804\n",
      "961 8593 remaining_3477802\n",
      "961 8594 remaining_3477803\n",
      "839 8685 rule_3597911\n",
      "256 8686 ruled_3597911\n",
      "609 8834 smelled_3893503\n",
      "372 8835 smelling_3893502\n",
      "367 8857 sorts_sort10700\n",
      "367 8858 sorts_sort12200\n",
      "200 8861 source_source11000\n",
      "200 8862 source_source11801\n",
      "200 8863 source_u\n",
      "316 8864 sources_source10900\n",
      "316 8865 sources_source11800\n",
      "740 8981 suspend_4155305\n",
      "139 9009 talk_4198507\n",
      "139 9010 talk_4198509\n",
      "507 9011 talked_4198508\n",
      "507 9012 talked_4198509\n",
      "276 9013 talking_4198502\n",
      "357 9125 treat_4380107\n",
      "2197 9128 treats_4380102\n",
      "2197 9129 treats_4380108\n",
      "0 9239 videodisc_disc10603s\n",
      "552 9268 wash_u\n",
      "802 9270 washing_4636104\n",
      "802 9271 washing_u\n",
      "278 9275 watched_4640508\n",
      "288 9318 win_4711406\n",
      "288 9319 win_u\n",
      "302 9330 won_4711406\n",
      "473 9344 wrote_4753408\n",
      "174 9653 activate_u\n",
      "461 9654 activates_u\n",
      "311 9655 activating_38204\n",
      "119 9662 add_u\n",
      "230 9663 adding_42604\n",
      "321 9667 adds_42602\n",
      "63 9964 argument_argument11002s\n",
      "104 9971 arm_arm10601\n",
      "271 10004 asking_238104\n",
      "1278 10005 asks_238103\n",
      "1278 10006 asks_238104\n",
      "1278 10007 asks_238106\n",
      "67 10156 bank_bank12101\n",
      "67 10157 bank_us\n",
      "93 10160 banks_u\n",
      "322 10249 begun_369204\n",
      "313 10912 climb_770005ers\n",
      "313 10913 climb_770005ing\n",
      "313 10914 climb_u\n",
      "180 10917 climbing_u\n",
      "1197 10918 climbs_770002\n",
      "898 11401 deciding_1067502\n",
      "44 11442 degree_degree10900\n",
      "44 11443 degree_degree11000s\n",
      "44 11444 degree_degree12300\n",
      "44 11445 degree_u\n",
      "305 11577 difficulties_difficulty10700\n",
      "306 11578 difficulty_u\n",
      "10980 11612 disappear_190901\n",
      "84 11621 disc_disc10603s\n",
      "122 11828 eat_1297002\n",
      "122 11829 eat_1297004\n",
      "122 11830 eat_u\n",
      "347 11831 eaten_u\n",
      "11195 11833 eats_1297001\n",
      "125 11944 encountered_u\n",
      "1836 11945 encountering_1353101\n",
      "136 12099 expected_u\n",
      "384 12125 express_u\n",
      "153 12126 expressed_1446803\n",
      "153 12127 expressed_u\n",
      "944 12129 expressing_1446803\n",
      "11988 12636 glasspaper_paper12700\n",
      "401 12937 hear_1892104\n",
      "401 12938 hear_1892106\n",
      "401 12939 hear_u\n",
      "283 12940 heard_1892103\n",
      "1681 12941 hearing_1892102\n",
      "12291 12944 hears_u\n",
      "859 12950 heat_1297001\n",
      "118 13083 hot_hot50000fast01\n",
      "118 13084 hot_hot50000good01\n",
      "118 13085 hot_hot50000illegal00\n",
      "118 13086 hot_hot50000sexy00\n",
      "92 13187 image_image11800\n",
      "291 13189 images_image11000\n",
      "140 13221 important_important50000immodest02\n",
      "140 13222 important_u\n",
      "71 13359 interest_interest11400\n",
      "726 13519 judgments_judgment11000\n",
      "0 13667 lang_out\n",
      "542 13683 late_1297001r\n",
      "362 13886 lose_2439907\n",
      "362 13887 lose_2439909\n",
      "1427 13888 loses_2439903\n",
      "829 13889 losing_2439905\n",
      "190 13890 lost_u\n",
      "0 13936 macdiarm_arm10800id\n",
      "187 14096 mean_2555507\n",
      "319 14097 means_2555501\n",
      "319 14098 means_u\n",
      "562 14099 meant_2555501\n",
      "562 14100 meant_2555503\n",
      "501 14226 miss_2644301\n",
      "501 14227 miss_2644305\n",
      "1432 14228 missing_2644304\n",
      "1432 14229 missing_2644305\n",
      "1432 14230 missing_2644306\n",
      "1080 14475 newspaper_paper11002\n",
      "1080 14476 newspaper_paper11003\n",
      "1080 14477 newspaper_paper11003s\n",
      "1080 14478 newspaper_paper11400s\n",
      "297 14563 note_2822011s\n",
      "297 14564 note_u\n",
      "867 14565 notes_2822011\n",
      "867 14566 notes_2822012\n",
      "4039 14674 operates_2893205\n",
      "162 14701 organization_organization10400\n",
      "162 14702 organization_organization10401\n",
      "162 14703 organization_organization10700\n",
      "7691 14770 overruled_3597906\n",
      "48 14830 paper_paper11001s\n",
      "132 14831 papers_paper10600\n",
      "98 15050 plan_plan10900ned\n",
      "221 15066 play_3165218er\n",
      "221 15067 play_u\n",
      "320 15068 played_3165219\n",
      "320 15069 played_3165220\n",
      "835 15072 plays_3165221\n",
      "5298 15165 postbank_bank11400\n",
      "141 15286 produced_3288303\n",
      "605 15287 produces_3288305\n",
      "299 15288 producing_3288306\n",
      "130 15343 provide_3313902d\n",
      "468 15344 provides_3313901\n",
      "468 15345 provides_3313903\n",
      "454 15346 providing_3313903\n",
      "454 15347 providing_3313906\n",
      "518 15522 receive_3434802\n",
      "326 15523 received_3434803\n",
      "5380 15525 receives_3434805\n",
      "1924 15526 receiving_3434806\n",
      "205 15629 remain_u\n",
      "250 15630 remained_u\n",
      "4156 15688 resource_source11500\n",
      "736 15833 rules_3597910\n",
      "2444 15834 ruling_3597906\n",
      "2444 15835 ruling_3597910\n",
      "2444 15836 ruling_3597911\n",
      "64 16151 shelter_shelter10600s\n",
      "64 16152 shelter_shelter10601-belts\n",
      "64 16153 shelter_shelter12100\n",
      "335 16155 shelters_shelter12100\n",
      "196 16244 simple_simple50000naive00\n",
      "609 16338 smelled_3893507\n",
      "1094 16339 smells_3893501\n",
      "1094 16340 smells_3893503\n",
      "1094 16341 smells_3893507\n",
      "166 16392 solid_solid50000cubic00\n",
      "166 16393 solid_solid50000frozen00\n",
      "166 16394 solid_solid50000opaque00\n",
      "166 16395 solid_solid50000wholesome00\n",
      "52 16410 sort_sort11800s\n",
      "316 16420 sources_source11801\n",
      "316 16421 sources_u\n",
      "740 16746 suspend_4155301\n",
      "740 16747 suspend_4155304\n",
      "111 16748 suspended_u\n",
      "1467 16749 suspending_4155301\n",
      "1467 16750 suspending_4155303\n",
      "1467 16751 suspending_4155305\n",
      "8401 16752 suspends_4155304\n",
      "8401 16753 suspends_4155305\n",
      "139 16830 talk_4198505\n",
      "507 16832 talked_4198503\n",
      "507 16833 talked_4198504\n",
      "276 16834 talking_4198503\n",
      "276 16835 talking_4198508\n",
      "276 16836 talking_u\n",
      "677 16837 talks_4198503\n",
      "357 17115 treat_4380103\n",
      "163 17116 treated_4380104\n",
      "163 17117 treated_u\n",
      "0 17253 unclimb_770005ed\n",
      "4320 17314 unimportant_important30004\n",
      "211 17407 use_4530704\n",
      "168 17408 used_4530703\n",
      "3461 17409 uses_4530701\n",
      "531 17411 using_4530702\n",
      "570 17567 warm_arm10800\n",
      "552 17574 wash_4636101-stand\n",
      "552 17575 wash_4636103\n",
      "552 17576 wash_4636106\n",
      "552 17577 wash_4636108\n",
      "309 17578 washed_4636102\n",
      "309 17579 washed_4636106\n",
      "309 17580 washed_4636112\n",
      "8677 17582 washes_4636101\n",
      "8677 17583 washes_u\n",
      "802 17584 washing_4636108\n",
      "802 17585 washing_4636112\n",
      "285 17589 watch_4640502\n",
      "285 17590 watch_4640508\n",
      "285 17591 watch_4640509\n",
      "278 17592 watched_4640502\n",
      "16810 17593 watches_4640507\n",
      "394 17594 watching_4640508\n",
      "288 17710 win_4711405\n",
      "803 17722 winning_4711403\n",
      "803 17723 winning_4711406\n",
      "352 17794 write_4753402r\n",
      "352 17795 write_4753404\n",
      "375 17796 writing_4753403\n",
      "358 17797 written_4753405\n",
      "358 17798 written_4753406\n",
      "358 17799 written_4753408\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'a neurophysiological level , the intensity of an experience is typically reflected in the number of neurones activated and , more specifically , in the firing frequency of the relevant neurones .',\n",
       "       'a neurophysiological level , the intensity of an experience is typically reflected in the number of neurones activated_38203 and , more specifically , in the firing frequency of the relevant neurones .',\n",
       "       'activated_38203', '2'],\n",
       "      dtype='<U376')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_vect.vocab.stoi['activated_38201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vect.vocab.stoi['activated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.1779\n",
       " 0.0653\n",
       "-0.2101\n",
       " 0.4058\n",
       "-0.3550\n",
       "-0.0813\n",
       " 0.2495\n",
       " 0.1174\n",
       "-0.0556\n",
       "-0.4036\n",
       "-0.0329\n",
       " 0.2610\n",
       " 0.0121\n",
       "-0.1832\n",
       " 0.1278\n",
       " 0.2207\n",
       " 0.2276\n",
       "-0.1625\n",
       " 0.0244\n",
       " 0.6215\n",
       " 0.3411\n",
       "-0.2268\n",
       "-0.4792\n",
       "-0.1314\n",
       "-0.4965\n",
       "-0.5004\n",
       " 0.0410\n",
       " 0.0429\n",
       "-0.5470\n",
       "-0.3433\n",
       " 0.0178\n",
       " 0.4539\n",
       "-0.1100\n",
       " 0.4652\n",
       "-0.1570\n",
       "-0.1195\n",
       "-0.5386\n",
       "-0.0097\n",
       "-0.2432\n",
       "-0.0703\n",
       " 0.1040\n",
       " 0.0233\n",
       " 0.1393\n",
       " 0.3932\n",
       " 0.1358\n",
       " 0.1178\n",
       "-0.0666\n",
       " 0.5072\n",
       "-0.0914\n",
       " 0.4563\n",
       "-0.0509\n",
       "-0.1137\n",
       " 0.1796\n",
       "-0.0665\n",
       "-0.3915\n",
       "-0.4298\n",
       "-0.2446\n",
       " 0.1243\n",
       "-0.0849\n",
       " 0.2075\n",
       "-0.0993\n",
       " 0.3063\n",
       " 0.0555\n",
       " 0.2950\n",
       "-0.4176\n",
       " 0.0673\n",
       "-0.2827\n",
       "-0.2683\n",
       "-0.2671\n",
       "-0.2508\n",
       "-0.0532\n",
       " 0.0129\n",
       " 0.4330\n",
       "-0.0217\n",
       "-0.1739\n",
       " 0.2382\n",
       " 0.0970\n",
       "-0.0024\n",
       "-0.3126\n",
       " 0.0277\n",
       " 0.3278\n",
       "-0.0733\n",
       "-0.6247\n",
       " 0.0684\n",
       "-0.4762\n",
       " 0.0779\n",
       " 0.0710\n",
       "-0.1981\n",
       "-0.0109\n",
       "-0.5299\n",
       "-0.1059\n",
       " 0.2035\n",
       " 0.1361\n",
       " 0.3058\n",
       " 0.3921\n",
       " 0.3208\n",
       " 0.1039\n",
       " 0.2070\n",
       "-0.3397\n",
       " 0.3328\n",
       "-0.1574\n",
       " 0.1099\n",
       " 0.3361\n",
       "-0.0109\n",
       " 0.4606\n",
       "-0.2200\n",
       "-0.5446\n",
       " 0.1798\n",
       "-0.0529\n",
       " 0.3537\n",
       "-0.0312\n",
       "-0.2038\n",
       " 0.2283\n",
       "-0.0690\n",
       " 0.0980\n",
       " 0.2214\n",
       " 0.2144\n",
       "-0.1412\n",
       "-0.3451\n",
       " 0.0552\n",
       " 0.0467\n",
       "-0.0377\n",
       "-0.8564\n",
       " 0.4675\n",
       "-0.2126\n",
       "-0.1759\n",
       " 0.0458\n",
       " 0.1658\n",
       " 0.6852\n",
       " 0.0187\n",
       "-0.3514\n",
       " 0.1357\n",
       "-0.2560\n",
       " 0.2396\n",
       "-0.3201\n",
       " 0.0399\n",
       "-0.0264\n",
       " 0.0255\n",
       "-0.4254\n",
       " 0.4696\n",
       " 0.2056\n",
       " 0.3935\n",
       "-0.1666\n",
       "-0.0591\n",
       " 0.0065\n",
       " 0.4996\n",
       " 0.5418\n",
       "-0.2068\n",
       "-0.2747\n",
       " 0.3679\n",
       " 0.4625\n",
       "-0.3290\n",
       "-0.1443\n",
       "-0.1281\n",
       " 0.2684\n",
       "-0.0182\n",
       " 0.0664\n",
       " 0.1678\n",
       " 0.0940\n",
       " 0.0475\n",
       "-0.1001\n",
       "-0.2617\n",
       "-0.5185\n",
       "-0.0059\n",
       "-0.1235\n",
       " 0.5441\n",
       "-0.1761\n",
       "-0.1771\n",
       " 0.1519\n",
       "-0.1492\n",
       "-0.4623\n",
       " 0.0260\n",
       "-0.1770\n",
       " 0.3480\n",
       " 0.1323\n",
       " 0.0195\n",
       "-0.3572\n",
       "-0.4241\n",
       "-0.5190\n",
       " 0.1750\n",
       "-0.4497\n",
       " 0.0111\n",
       " 0.1589\n",
       "-0.3845\n",
       "-0.0670\n",
       " 0.6378\n",
       "-0.2901\n",
       " 0.1053\n",
       "-0.3038\n",
       "-0.1335\n",
       "-0.1105\n",
       "-0.4769\n",
       "-0.0123\n",
       "-0.0639\n",
       "-0.4663\n",
       "-0.2127\n",
       " 0.0673\n",
       " 0.2539\n",
       " 0.0678\n",
       " 0.1289\n",
       "-0.0150\n",
       "-0.3901\n",
       "-0.0999\n",
       "-0.2224\n",
       " 0.4671\n",
       "-0.4235\n",
       "-0.0942\n",
       " 0.2943\n",
       "-0.1405\n",
       " 0.3169\n",
       " 0.3582\n",
       "-0.1899\n",
       " 0.1932\n",
       " 0.3166\n",
       " 0.0621\n",
       " 0.0487\n",
       " 0.1499\n",
       "-0.6397\n",
       "-0.4096\n",
       " 0.3593\n",
       "-0.2225\n",
       " 0.0685\n",
       "-0.2415\n",
       " 0.1462\n",
       "-0.1783\n",
       "-0.1550\n",
       "-0.0419\n",
       "-0.3290\n",
       "-0.5956\n",
       " 0.2626\n",
       " 0.1936\n",
       "-0.4654\n",
       "-0.2151\n",
       "-0.0765\n",
       "-0.0668\n",
       " 0.3294\n",
       "-0.0829\n",
       " 0.0358\n",
       " 0.3546\n",
       "-0.0738\n",
       " 0.2961\n",
       " 0.3903\n",
       " 0.1079\n",
       " 0.0642\n",
       "-0.1841\n",
       " 0.1613\n",
       "-0.5378\n",
       "-0.0846\n",
       " 0.2860\n",
       " 0.1926\n",
       "-0.3996\n",
       " 0.0283\n",
       " 0.0949\n",
       "-0.0197\n",
       " 0.0727\n",
       "-0.1150\n",
       "-0.3209\n",
       "-0.2477\n",
       "-0.1490\n",
       "-0.0235\n",
       " 0.1146\n",
       "-0.5321\n",
       "-0.1180\n",
       " 0.0289\n",
       "-0.1622\n",
       "-0.3558\n",
       " 0.0213\n",
       " 0.1028\n",
       "-0.0269\n",
       " 0.1062\n",
       "-0.3473\n",
       "-0.2513\n",
       " 0.2111\n",
       "-0.2226\n",
       "-0.1924\n",
       "-0.3036\n",
       "-0.0425\n",
       "-0.2102\n",
       " 0.3563\n",
       "-0.4508\n",
       " 0.2100\n",
       " 0.0305\n",
       "-0.2260\n",
       "-0.0604\n",
       " 0.3150\n",
       "-0.3127\n",
       " 0.4912\n",
       " 0.0736\n",
       "-0.3070\n",
       "-0.0415\n",
       "-0.1044\n",
       "-0.1124\n",
       "-0.1445\n",
       " 0.0886\n",
       "-0.2161\n",
       "-0.3577\n",
       " 0.1652\n",
       " 0.5670\n",
       " 0.3353\n",
       "-0.3828\n",
       "[torch.FloatTensor of size 300]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vect.vocab.vectors[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.1779\n",
       " 0.0653\n",
       "-0.2101\n",
       " 0.4058\n",
       "-0.3550\n",
       "-0.0813\n",
       " 0.2495\n",
       " 0.1174\n",
       "-0.0556\n",
       "-0.4036\n",
       "-0.0329\n",
       " 0.2610\n",
       " 0.0121\n",
       "-0.1832\n",
       " 0.1278\n",
       " 0.2207\n",
       " 0.2276\n",
       "-0.1625\n",
       " 0.0244\n",
       " 0.6215\n",
       " 0.3411\n",
       "-0.2268\n",
       "-0.4792\n",
       "-0.1314\n",
       "-0.4965\n",
       "-0.5004\n",
       " 0.0410\n",
       " 0.0429\n",
       "-0.5470\n",
       "-0.3433\n",
       " 0.0178\n",
       " 0.4539\n",
       "-0.1100\n",
       " 0.4652\n",
       "-0.1570\n",
       "-0.1195\n",
       "-0.5386\n",
       "-0.0097\n",
       "-0.2432\n",
       "-0.0703\n",
       " 0.1040\n",
       " 0.0233\n",
       " 0.1393\n",
       " 0.3932\n",
       " 0.1358\n",
       " 0.1178\n",
       "-0.0666\n",
       " 0.5072\n",
       "-0.0914\n",
       " 0.4563\n",
       "-0.0509\n",
       "-0.1137\n",
       " 0.1796\n",
       "-0.0665\n",
       "-0.3915\n",
       "-0.4298\n",
       "-0.2446\n",
       " 0.1243\n",
       "-0.0849\n",
       " 0.2075\n",
       "-0.0993\n",
       " 0.3063\n",
       " 0.0555\n",
       " 0.2950\n",
       "-0.4176\n",
       " 0.0673\n",
       "-0.2827\n",
       "-0.2683\n",
       "-0.2671\n",
       "-0.2508\n",
       "-0.0532\n",
       " 0.0129\n",
       " 0.4330\n",
       "-0.0217\n",
       "-0.1739\n",
       " 0.2382\n",
       " 0.0970\n",
       "-0.0024\n",
       "-0.3126\n",
       " 0.0277\n",
       " 0.3278\n",
       "-0.0733\n",
       "-0.6247\n",
       " 0.0684\n",
       "-0.4762\n",
       " 0.0779\n",
       " 0.0710\n",
       "-0.1981\n",
       "-0.0109\n",
       "-0.5299\n",
       "-0.1059\n",
       " 0.2035\n",
       " 0.1361\n",
       " 0.3058\n",
       " 0.3921\n",
       " 0.3208\n",
       " 0.1039\n",
       " 0.2070\n",
       "-0.3397\n",
       " 0.3328\n",
       "-0.1574\n",
       " 0.1099\n",
       " 0.3361\n",
       "-0.0109\n",
       " 0.4606\n",
       "-0.2200\n",
       "-0.5446\n",
       " 0.1798\n",
       "-0.0529\n",
       " 0.3537\n",
       "-0.0312\n",
       "-0.2038\n",
       " 0.2283\n",
       "-0.0690\n",
       " 0.0980\n",
       " 0.2214\n",
       " 0.2144\n",
       "-0.1412\n",
       "-0.3451\n",
       " 0.0552\n",
       " 0.0467\n",
       "-0.0377\n",
       "-0.8564\n",
       " 0.4675\n",
       "-0.2126\n",
       "-0.1759\n",
       " 0.0458\n",
       " 0.1658\n",
       " 0.6852\n",
       " 0.0187\n",
       "-0.3514\n",
       " 0.1357\n",
       "-0.2560\n",
       " 0.2396\n",
       "-0.3201\n",
       " 0.0399\n",
       "-0.0264\n",
       " 0.0255\n",
       "-0.4254\n",
       " 0.4696\n",
       " 0.2056\n",
       " 0.3935\n",
       "-0.1666\n",
       "-0.0591\n",
       " 0.0065\n",
       " 0.4996\n",
       " 0.5418\n",
       "-0.2068\n",
       "-0.2747\n",
       " 0.3679\n",
       " 0.4625\n",
       "-0.3290\n",
       "-0.1443\n",
       "-0.1281\n",
       " 0.2684\n",
       "-0.0182\n",
       " 0.0664\n",
       " 0.1678\n",
       " 0.0940\n",
       " 0.0475\n",
       "-0.1001\n",
       "-0.2617\n",
       "-0.5185\n",
       "-0.0059\n",
       "-0.1235\n",
       " 0.5441\n",
       "-0.1761\n",
       "-0.1771\n",
       " 0.1519\n",
       "-0.1492\n",
       "-0.4623\n",
       " 0.0260\n",
       "-0.1770\n",
       " 0.3480\n",
       " 0.1323\n",
       " 0.0195\n",
       "-0.3572\n",
       "-0.4241\n",
       "-0.5190\n",
       " 0.1750\n",
       "-0.4497\n",
       " 0.0111\n",
       " 0.1589\n",
       "-0.3845\n",
       "-0.0670\n",
       " 0.6378\n",
       "-0.2901\n",
       " 0.1053\n",
       "-0.3038\n",
       "-0.1335\n",
       "-0.1105\n",
       "-0.4769\n",
       "-0.0123\n",
       "-0.0639\n",
       "-0.4663\n",
       "-0.2127\n",
       " 0.0673\n",
       " 0.2539\n",
       " 0.0678\n",
       " 0.1289\n",
       "-0.0150\n",
       "-0.3901\n",
       "-0.0999\n",
       "-0.2224\n",
       " 0.4671\n",
       "-0.4235\n",
       "-0.0942\n",
       " 0.2943\n",
       "-0.1405\n",
       " 0.3169\n",
       " 0.3582\n",
       "-0.1899\n",
       " 0.1932\n",
       " 0.3166\n",
       " 0.0621\n",
       " 0.0487\n",
       " 0.1499\n",
       "-0.6397\n",
       "-0.4096\n",
       " 0.3593\n",
       "-0.2225\n",
       " 0.0685\n",
       "-0.2415\n",
       " 0.1462\n",
       "-0.1783\n",
       "-0.1550\n",
       "-0.0419\n",
       "-0.3290\n",
       "-0.5956\n",
       " 0.2626\n",
       " 0.1936\n",
       "-0.4654\n",
       "-0.2151\n",
       "-0.0765\n",
       "-0.0668\n",
       " 0.3294\n",
       "-0.0829\n",
       " 0.0358\n",
       " 0.3546\n",
       "-0.0738\n",
       " 0.2961\n",
       " 0.3903\n",
       " 0.1079\n",
       " 0.0642\n",
       "-0.1841\n",
       " 0.1613\n",
       "-0.5378\n",
       "-0.0846\n",
       " 0.2860\n",
       " 0.1926\n",
       "-0.3996\n",
       " 0.0283\n",
       " 0.0949\n",
       "-0.0197\n",
       " 0.0727\n",
       "-0.1150\n",
       "-0.3209\n",
       "-0.2477\n",
       "-0.1490\n",
       "-0.0235\n",
       " 0.1146\n",
       "-0.5321\n",
       "-0.1180\n",
       " 0.0289\n",
       "-0.1622\n",
       "-0.3558\n",
       " 0.0213\n",
       " 0.1028\n",
       "-0.0269\n",
       " 0.1062\n",
       "-0.3473\n",
       "-0.2513\n",
       " 0.2111\n",
       "-0.2226\n",
       "-0.1924\n",
       "-0.3036\n",
       "-0.0425\n",
       "-0.2102\n",
       " 0.3563\n",
       "-0.4508\n",
       " 0.2100\n",
       " 0.0305\n",
       "-0.2260\n",
       "-0.0604\n",
       " 0.3150\n",
       "-0.3127\n",
       " 0.4912\n",
       " 0.0736\n",
       "-0.3070\n",
       "-0.0415\n",
       "-0.1044\n",
       "-0.1124\n",
       "-0.1445\n",
       " 0.0886\n",
       "-0.2161\n",
       "-0.3577\n",
       " 0.1652\n",
       " 0.5670\n",
       " 0.3353\n",
       "-0.3828\n",
       "[torch.FloatTensor of size 300]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_vect.vocab.vectors[97]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading senseval 3 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_instance(text):\n",
    "    pairs = []\n",
    "    \n",
    "    sense_ids = re.findall(r'senseid=\\\"(.*?)\\\"', text, re.DOTALL)\n",
    "    context = re.findall(r'<context>(.*?)</context>', text, re.DOTALL)\n",
    "    word_ambiguos = re.findall(r'<head>(.*?)</head>', context[0], re.DOTALL)\n",
    "    sentences = re.findall(r'(.*?)\\.', context[0])\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tags = re.findall(r'<head>(.*?)</head>', sentence)\n",
    "        if(len(tags) != 0):\n",
    "            for sense_id in sense_ids:   \n",
    "                pair = [[],[]]\n",
    "                sense_id = re.sub(r'%|:', '', sense_id)\n",
    "                pair[0] = re.sub(r'<head>(.*?)</head>', word_ambiguos[0], sentence)\n",
    "                pair[1] = re.sub(r'<head>(.*?)</head>', word_ambiguos[0] + '_' + sense_id, sentence)\n",
    "                pairs.append(pair)\n",
    "        \n",
    "    return np.array(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_pairs(path):\n",
    "    with open('corpus/EnglishLS.train', 'r') as f:\n",
    "        xml = f.read()\n",
    "\n",
    "    instances = re.findall(r'<instance(.*?)</instance>', xml, re.DOTALL)\n",
    "    pairs= []\n",
    "\n",
    "    for instance in instances:\n",
    "        data = \"<instance\" + instance + \"</instance>\"\n",
    "        data = re.sub(r'[^\\x20-\\x7E]', '', data)\n",
    "        data = re.sub(r' n\\'t', 'n\\'t', data)\n",
    "        data = re.sub(r'&', '', data)\n",
    "        pairs.extend(process_instance(data))\n",
    "        \n",
    "    return np.array(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = construct_pairs('corpus/EnglishLS.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '  It is quite a hefty spade , with bicycle - type handlebars and a sprung lever at the rear , which you step on to activate it ',\n",
       "       '  It is quite a hefty spade , with bicycle - type handlebars and a sprung lever at the rear , which you step on to activate_38201 it '],\n",
       "      dtype='<U1335')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.trimmed = False\n",
    "        self.stoi = {}\n",
    "        self.word2count = {}\n",
    "        self.itos = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.stoi:\n",
    "            self.stoi[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.itos[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.stoi), len(keep_words) / len(self.stoi)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.stoi = {}\n",
    "        self.word2count = {}\n",
    "        self.itos = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_pairs(pairs):\n",
    "    for pair in pairs:\n",
    "        pair[0] = normalize_string(pair[0])\n",
    "        pair[1] = normalize_string(pair[1])\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    filtered_pairs = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) <= MAX_LENGTH \\\n",
    "            and len(pair[1]) <= MAX_LENGTH:\n",
    "                filtered_pairs.append(pair)\n",
    "    return filtered_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(pairs):\n",
    "    \n",
    "    normalize_pairs(pairs)\n",
    "    print(\"Reading pairs %d\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Filtered to %d pairs\" % len(pairs))\n",
    "    \n",
    "    sentence =  Lang()\n",
    "    sense = Lang()\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        sentence.index_words(pair[0])\n",
    "        sense.index_words(pair[1])\n",
    "    \n",
    "    print('Indexed %d words in input language, %d words in output' % (sentence.n_words, sense.n_words))\n",
    "    return sentence, sense, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pairs 8453\n",
      "Filtered to 8452 pairs\n",
      "Indexing words...\n",
      "Indexed 20843 words in input language, 21553 words in output\n"
     ]
    }
   ],
   "source": [
    "sentence, sense, pairs = prepare_data(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'it is quite a hefty spade , with bicycle - type handlebars and a sprung lever at the rear , which you step on to activate it',\n",
       "       'it is quite a hefty spade , with bicycle - type handlebars and a sprung lever at the rear , which you step on to activate_38201 it'],\n",
       "      dtype='<U1335')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.stoi[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(sentence, pair[0]))\n",
    "        target_seqs.append(indexes_from_sentence(sense, pair[1]))\n",
    "\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        input_seqs, input_lengths, hidden = input\n",
    "        \n",
    "        embedded = self.embedding(input_seqs)\n",
    "        \n",
    "        #packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        self.gru.flatten_parameters() \n",
    "        outputs, hidden = self.gru(embedded, hidden)      \n",
    "        #outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        \n",
    "        padded_output = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return padded_output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy =torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = energy.view(-1,1)\n",
    "            energy = hidden.mm(energy)\n",
    "            return energy\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = torch.dot(self.v.view(-1), energy.view(-1))\n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "        print(\"hidden layer\")\n",
    "        print(last_hidden)\n",
    "        \n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(n_layers, input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder((input_batches, input_lengths, None))\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, sense.n_words))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "        \n",
    "        del decoder_output\n",
    "        decoder_output = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del decoder_hidden\n",
    "    torch.cuda.empty_cache()\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    loss.backward()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure models\n",
    "attn_model = 'general'\n",
    "hidden_size = 300\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 10\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 50000\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "print_every = 300\n",
    "evaluate_every = 1000\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(sentence.n_words, hidden_size, n_layers, dropout=dropout)\n",
    "#encoder = torch.nn.DistributedDataParallel(encoder, device_ids=[0, 1])\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, sense.n_words, n_layers, dropout=dropout)\n",
    "#decoder = torch.nn.DistributedDataParallel(decoder, device_ids=[0, 1])\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    #encoder = nn.DataParallel(encoder, device_ids=[0,1]).cuda()\n",
    "    #decoder = nn.DataParallel(decoder, device_ids=[0,1]).cuda()\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.0511  0.2237 -0.2490  ...   0.0491  0.3095 -0.1848\n",
      " -0.3700  0.0742  0.2353  ...  -0.2486  0.3134  0.4576\n",
      " -0.4135 -0.1339  0.6474  ...  -0.3013  0.4225  0.6595\n",
      "           ...                          ...          \n",
      " -0.4248 -0.1582  0.6511  ...  -0.3235  0.4222  0.6633\n",
      " -0.4250 -0.1589  0.6510  ...  -0.3229  0.4222  0.6634\n",
      " -0.4251 -0.1598  0.6511  ...  -0.3228  0.4221  0.6632\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.6223 -0.2182  0.2976  ...   0.5492  0.2573  0.4666\n",
      " -0.2972  0.2671 -0.4795  ...   0.3262 -0.2472  0.2179\n",
      "  0.3301  0.3254  0.1263  ...  -0.4029  0.1811  0.0198\n",
      "           ...                          ...          \n",
      "  0.2214  0.3348 -0.5984  ...   0.5105 -0.3768  0.3825\n",
      " -0.3032  0.2465 -0.5671  ...   0.1797 -0.4157  0.4108\n",
      " -0.1247  0.2693 -0.3294  ...   0.5356  0.1254  0.4586\n",
      "[torch.cuda.FloatTensor of size 2x10x300 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (2, 1, 300), got (2, 10, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-81e3551d0ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2bc4c8e88b39>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_layers, input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         decoder_output, decoder_hidden, decoder_attn = decoder(\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-7365a7584661>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Get current hidden state from input word and last hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Calculate attention from current RNN state and all encoder outputs;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mflat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[0;34m(self, input, weight, hx)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a20112128/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             raise RuntimeError('Expected hidden size {}, got {}'.format(\n\u001b[0;32m--> 266\u001b[0;31m                 hidden_size, tuple(hx.size())))\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             raise RuntimeError('Expected cell size {}, got {}'.format(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden size (2, 1, 300), got (2, 10, 300)"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths = random_batch(batch_size)\n",
    "\n",
    "    # Run the train function\n",
    "    loss, ec, dc = train(\n",
    "        n_layers, input_batches, input_lengths, target_batches, target_lengths,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer, criterion\n",
    "    )\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "    \n",
    "    del loss\n",
    "    del input_batches\n",
    "    del target_batches\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        \n",
    "    #if epoch % evaluate_every == 0:\n",
    "    #    evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        # TODO: Running average helper\n",
    "        ecs.append(eca / plot_every)\n",
    "        dcs.append(dca / plot_every)\n",
    "        eca = 0\n",
    "        dca = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(input_seq, max_length=MAX_LENGTH):   \n",
    "    input_seqs = [indexes_from_sentence(sentence, input_seq)]\n",
    "    input_lengths = [len(input_seqs)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(sense.itos[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        #if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    [input_sentence, target_sentence] = random.choice(pairs)\n",
    "    evaluate_and_show_attention(input_sentence, target_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "def show_plot_visdom():\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    attn_win = 'attention (%s)' % hostname\n",
    "    vis.image(torchvision.transforms.ToTensor()(Image.open(buf)), win=attn_win, opts={'title': attn_win})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    \n",
    "    #show_attention(input_sentence, output_words, attentions)\n",
    "    \n",
    "    # Show input, target, output text in visdom\n",
    "    #win = 'evaluted'\n",
    "    #text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
    "    #vis.text(text, win=win, opts={'title': win})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
