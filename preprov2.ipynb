{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from tqdm import tqdm\n",
    "import corenlp\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import enchant\n",
    "\n",
    "os.environ[\"CORENLP_HOME\"] = '/home/krivas/projects/neural-wsd/new_experiments/data/lib/stanford-corenlp'\n",
    "\n",
    "def join_words(sentences, word_dict):\n",
    "    arr_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        ant = ''\n",
    "        new_sentence = ''\n",
    "        add_word = True\n",
    "        \n",
    "        for ix, token in enumerate(tokens):\n",
    "            if(add_word):\n",
    "                if(token == '-'  and ix > 0 and ix < (len(tokens) - 1)):\n",
    "                    join_word = tokens[ix-1] + '-' + tokens[ix+1]\n",
    "                    if word_dict.check(join_word):\n",
    "                        ant = join_word + ' '\n",
    "                        add_word = False\n",
    "\n",
    "                if(add_word):\n",
    "                    new_sentence += ant\n",
    "                    ant = token + \" \"\n",
    "            else:\n",
    "                add_word = True\n",
    "\n",
    "        new_sentence += ant\n",
    "        arr_sentences.append(new_sentence)\n",
    "    \n",
    "    return arr_sentences\n",
    "\n",
    "def remove_unnecesary_char(sentence):\n",
    "    sentence = sentence.strip(' ')\n",
    "    sentence = sentence.lstrip(')')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def find_index(sentence):\n",
    "    words = sentence.split()\n",
    "    \n",
    "    lst_index = []\n",
    "    for ix_word, word in enumerate(words):\n",
    "        if '<head>' in word:\n",
    "            lst_index.append(ix_word)\n",
    "\n",
    "    return lst_index\n",
    "\n",
    "def find_trim_sentence(sentence, iz_del, der_del):\n",
    "    cont_iz = 0\n",
    "    seqs = []\n",
    "    seq = ''\n",
    "    tokens = sentence.split()\n",
    "    add = False\n",
    "    \n",
    "    for token in tokens:\n",
    "        if iz_del in token:\n",
    "            cont_iz += 1\n",
    "        \n",
    "        if cont_iz > 0:\n",
    "            add = True\n",
    "            seq += token + ' '\n",
    "        \n",
    "        if der_del in token and cont_iz:\n",
    "            cont_iz -= 1\n",
    "        \n",
    "        if cont_iz == 0 and add:\n",
    "            seq = seq.strip(' ')\n",
    "            seqs.append(seq)\n",
    "            seq = ''\n",
    "            add = False\n",
    "            \n",
    "    if cont_iz:\n",
    "        seqs.append(seq)\n",
    "            \n",
    "    return seqs\n",
    "\n",
    "def remove_LRB_RRB(sentences, iz_del, der_del):\n",
    "    arr_sentence = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        seqs = find_trim_sentence(sentence, iz_del, der_del)\n",
    "        remove = True\n",
    "        for seq in seqs:\n",
    "            arr = find_index(seq)\n",
    "            if len(arr):\n",
    "                sentence = seq.strip(iz_del + der_del + ' ')\n",
    "                remove = False\n",
    "                break\n",
    "\n",
    "        if remove:\n",
    "            sentence = re.sub(r'\\((.*?)\\)', '', sentence, re.DOTALL)\n",
    "        if ')' not in sentence.split()[-1]:\n",
    "            sentence = re.sub(r'(.*?)\\)', '', sentence, re.DOTALL)\n",
    "        else:\n",
    "            sentence = re.sub(r'\\)', '', sentence, re.DOTALL) \n",
    "\n",
    "        if '(' not in sentence.split()[0]:\n",
    "            sentence += ' .'\n",
    "            sentence = re.sub(r'\\((.*?)\\.', '', sentence, re.DOTALL) \n",
    "        else:\n",
    "            sentence = re.sub(r'\\(', '', sentence, re.DOTALL)     \n",
    "        arr_sentence.append(sentence)\n",
    "    \n",
    "    return arr_sentence\n",
    "\n",
    "def process_instance(file, word_target, text, word_dict, sense_ids=None, tokenizer=None, verbose=False):\n",
    "    pairs = []\n",
    "    sentences = []\n",
    "    \n",
    "    if not sense_ids:\n",
    "        sense_ids = re.findall(r'senseid=\\\"(.*?)\\\"', text, re.DOTALL)\n",
    "        is_test = False\n",
    "    else:\n",
    "        is_test = True\n",
    "        \n",
    "    context = re.findall(r'<context>(.*?)</context>', text, re.DOTALL)\n",
    "    word_ambiguos = re.findall(r'<head>(.*?)</head>', context[0], re.DOTALL)\n",
    "    \n",
    "    sentences = re.split(r'[\\.|:|?|!]', context[0])    \n",
    "    for sentence in sentences:\n",
    "        if '<head>' in sentence:\n",
    "            sentence = remove_unnecesary_char(sentence)     \n",
    "            prunes = remove_LRB_RRB([sentence], '(', ')')\n",
    "            \n",
    "            if(verbose):\n",
    "                print('---oracion')\n",
    "                print(sentence)\n",
    "                print('---oracion sin parentesis')\n",
    "                print(prunes[0])\n",
    "                print('\\n')\n",
    "                \n",
    "            for prune in prunes:\n",
    "                prune = join_words([prune], word_dict)[0] + ' .'\n",
    "                prune = re.sub('<head>', ' <head>', prune)\n",
    "                prune = re.sub('</head>', '</head> ', prune)\n",
    "                ann = tokenizer.annotate(prune)\n",
    "                prune = ' '.join([w.word for w in ann.sentence[0].token])\n",
    "                index_word = find_index(prune) \n",
    "\n",
    "                for sense_id in sense_ids:   \n",
    "                    pair = [[],[],[],[]]\n",
    "                    sense_id = re.sub(r'%|:', '', sense_id)\n",
    "                    pair[0] = ' '.join(re.sub(r'<head>(.*?)</head>', ' ' + word_ambiguos[0] + ' ', prune).split())\n",
    "                    pair[1] = ' '.join(re.sub(r'<head>(.*?)</head>', ' ' + word_target + '_' + sense_id + ' ', prune).split())\n",
    "                    pair[2] = word_target + '_' + sense_id\n",
    "                    pair[3] = index_word\n",
    "                    pairs.append(pair)\n",
    "                    \n",
    "                    # Saving in a file\n",
    "                    if not is_test:\n",
    "                        file.write(pair[0] + '\\t' + pair[1] + '\\n')\n",
    "                if is_test:\n",
    "                    index_word = str(index_word[0] if len(index_word) else index_word)\n",
    "                    file.write(pair[0] + '\\t' + index_word)\n",
    "                    for pair in pairs:\n",
    "                        file.write('\\t' + pair[2])\n",
    "                    file.write('\\n')\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "def load_senses(path_senses, path_test):\n",
    "    \n",
    "    targets_all = []\n",
    "    with open(path_test, 'r', encoding='iso-8859-1') as f:\n",
    "        xml = f.read()  \n",
    "    \n",
    "    instances = re.findall(r'<instance(.*?)</instance>', xml, re.DOTALL)\n",
    "    for ix_ins, instance in enumerate(instances):\n",
    "        data = '<instance' + instance + '</instance>'\n",
    "        senses_ids = re.findall(r'<head>', data, re.DOTALL)\n",
    "        targets_all.append(len(senses_ids))\n",
    "    \n",
    "    senses_all = []\n",
    "    with open(path_senses, 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        for line in lines:\n",
    "            senses = []\n",
    "            words = line.split()\n",
    "            for ix, word in enumerate(words):\n",
    "                if ix > 1:\n",
    "                    word = re.sub(r'%|:', '', word)\n",
    "                    senses.append(word)\n",
    "                    \n",
    "            senses_all.append(senses)\n",
    "    \n",
    "    return senses_all, targets_all\n",
    "\n",
    "def replace_characters(instance):\n",
    "    \n",
    "    data = '<instance' + instance + '</instance>'\n",
    "    data = re.sub(r'[^\\x20-\\x7E]', '', data)\n",
    "    data = re.sub(r' n\\'t', 'n\\'t', data)\n",
    "    data = re.sub(r'wou \\'d', 'uld', data)\n",
    "\n",
    "    data = re.sub(r' \\'re', ' are', data)\n",
    "    data = re.sub(r' \\'ve', ' have', data)\n",
    "\n",
    "    data = re.sub(r'it \\'s', 'it is', data)\n",
    "    data = re.sub(r'he \\'s', 'he is', data)\n",
    "    data = re.sub(r'i \\'m', 'i am', data)\n",
    "    data = re.sub(r'It \\'s', 'it is', data)\n",
    "    data = re.sub(r'He \\'s', 'he is', data)\n",
    "    data = re.sub(r'I \\'m', 'i am', data)\n",
    "\n",
    "    data = re.sub(r'\\[(.*?)\\]', '', data)\n",
    "    data = re.sub(r'&(.*?);', '', data)\n",
    "\n",
    "    data = re.sub(r' \\'d', 'd', data)\n",
    "    data = re.sub(r'&', '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def construct_pairs(path_source, path_model, test_path = None, tokenizer = None, verbose=True, name_file='input.raw'):\n",
    "    \n",
    "    word_dict = enchant.Dict('en_US')\n",
    "    \n",
    "    with open(path_source, 'r', encoding='iso-8859-1') as f:\n",
    "        xml = f.read()\n",
    "\n",
    "    if test_path:\n",
    "        senses_all, _ = load_senses(test_path, path_source)    \n",
    "    \n",
    "    lexelts = re.findall(r'<lexelt(.*?)</lexelt>', xml, re.DOTALL)\n",
    "    pairs= []\n",
    "    ix_ins = 0\n",
    "    with open(os.path.dirname(path_source) + f'/{name_file}', 'w') as file:\n",
    "        for ix_lex, lexelt in enumerate(lexelts):\n",
    "            item = re.findall(r'item=\\\"(.*?)\\\"', lexelt, re.DOTALL)\n",
    "            word_target = item[0].split('.')[0]\n",
    "\n",
    "            instances = re.findall(r'<instance(.*?)</instance>', lexelt, re.DOTALL)\n",
    "            for instance in instances:\n",
    "                data = replace_characters(instance)\n",
    "                \n",
    "                context = re.findall(r'<context>(.*?)</context>', data, re.DOTALL)\n",
    "\n",
    "                if not test_path:\n",
    "                    pairs.extend(process_instance(file, word_target, data, word_dict, None, tokenizer, verbose))\n",
    "                else:\n",
    "                    pairs.extend(process_instance(file, word_target, data, word_dict, senses_all[ix_ins], tokenizer, verbose))\n",
    "                ix_ins += 1\n",
    "    return pairs\n",
    "\n",
    "def make_dirs(dirs):\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "            \n",
    "def dependency_parse(filepath,  client, tokenize=True):\n",
    "    print('\\nDependency parsing ' + filepath)\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    filepre = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    parentpath = os.path.join(dirpath, filepre + '.parents')\n",
    "    deps = []\n",
    "    with open(filepath) as file:\n",
    "        for line in tqdm(file, total=file.tell()):\n",
    "            temp = client.dependency_parse(line)\n",
    "            temp = list(map(lambda x: [int(x[1]), int(x[2])], temp))\n",
    "            temp = list(itertools.chain(*temp))\n",
    "            deps.append(temp)\n",
    "    np.save(parentpath, np.array(deps))\n",
    "\n",
    "def build_vocab(filepaths, dst_path, lowercase=True):\n",
    "    vocab = set()\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath) as f:\n",
    "            for line in f:\n",
    "                if lowercase:\n",
    "                    line = line.lower()\n",
    "                vocab |= set(line.split())\n",
    "    with open(dst_path, 'w') as f:\n",
    "        for w in sorted(vocab):\n",
    "            f.write(w + '\\n')\n",
    "\n",
    "def split(filepath, dst_dir, client):\n",
    "    with open(filepath) as datafile, \\\n",
    "            open(os.path.join(dst_dir, 'in.txt'), 'w') as afile, \\\n",
    "            open(os.path.join(dst_dir, 'out.txt'), 'w') as bfile:\n",
    "        datafile.readline()\n",
    "        for line in datafile:\n",
    "            a, b = line.strip().split('\\t')\n",
    "\n",
    "            ann = client.annotate(a)\n",
    "            s = ' '.join([w.word for w in ann.sentence[0].token])\n",
    "            afile.write(s + '\\n')\n",
    "                \n",
    "            ann = client.annotate(b)\n",
    "            s = ' '.join([w.word for w in ann.sentence[0].token])\n",
    "            bfile.write(s + '\\n')\n",
    "\n",
    "def parse(dirpath, client):\n",
    "    dependency_parse(os.path.join(dirpath, 'in.txt'), client, tokenize=True)\n",
    "    #dependency_parse(os.path.join(dirpath, 'out.txt'), client, cp=cp, tokenize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Preprocessing dataset\n",
      "================================================================================\n",
      "Construction train data\n",
      "Construction test data\n",
      "Splitting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data\n",
      "\n",
      "Dependency parsing data/disambiguation/sense-eval2/train/in.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'itertools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-92dbf6d9b4ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# parse sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Parsing data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-171-2db4f164ae42>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(dirpath, client)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mdependency_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;31m#dependency_parse(os.path.join(dirpath, 'out.txt'), client, cp=cp, tokenize=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-2db4f164ae42>\u001b[0m in \u001b[0;36mdependency_parse\u001b[0;34m(filepath, client, tokenize)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependency_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparentpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itertools' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    print('=' * 80)\n",
    "    print('Preprocessing dataset')\n",
    "    print('=' * 80)\n",
    "    base_dir = ''\n",
    "    data_dir = os.path.join(base_dir, 'data')\n",
    "    lib_dir = os.path.join(data_dir, 'lib')\n",
    "    client_tree = StanfordCoreNLP(os.path.join(lib_dir, 'stanford-corenlp'))\n",
    "    client_tok = corenlp.CoreNLPClient(annotators=\"tokenize ssplit\".split())\n",
    "    \n",
    "    #Processing corpus train\n",
    "    print('Construction train data')\n",
    "    construct_pairs('data/disambiguation/sense-eval3/train/EnglishLS.train', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                              test_path=None, tokenizer=client_tok, verbose=False, name_file='train.raw')\n",
    "    construct_pairs('data/disambiguation/sense-eval2/train/eng-lex-sample.training.xml', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                              test_path=None, tokenizer=client_tok, verbose=False, name_file='train.raw')\n",
    "    #Processing corpus test\n",
    "    print('Construction test data')\n",
    "    construct_pairs('data/disambiguation/sense-eval3/test/EnglishLS.test', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                             test_path='data/disambiguation/sense-eval3/test/EnglishLS.test.key', tokenizer=client_tok, verbose=False, name_file='test.raw')\n",
    "    construct_pairs('data/disambiguation/sense-eval2/test/eng-lex-samp.evaluation.xml', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                             test_path='data/disambiguation/sense-eval2/test/key.txt', tokenizer=client_tok, verbose=False, name_file='test.raw')\n",
    "    \n",
    "    senses_dir=['sense-eval2', 'sense-eval3']\n",
    "    for sense_dir in senses_dir:\n",
    "        train_dir = os.path.join(data_dir, f'disambiguation/{sense_dir}/train')\n",
    "        test_dir = os.path.join(data_dir, f'disambiguation/{sense_dir}/test')\n",
    "\n",
    "        # split into separate files\n",
    "        print('Splitting data')\n",
    "        split(os.path.join(train_dir, 'train.raw'), train_dir, client_tok)\n",
    "        #split(os.path.join(test_dir, 'test.raw'), test_dir, client_tok)\n",
    "\n",
    "        # parse sentences\n",
    "        print('Parsing data')\n",
    "        parse(train_dir, client_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
