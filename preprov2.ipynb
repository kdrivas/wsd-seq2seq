{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from tqdm import tqdm\n",
    "import corenlp\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import enchant\n",
    "\n",
    "os.environ[\"CORENLP_HOME\"] = '/home/krivas/projects/neural-wsd/new_experiments/data/lib/stanford-corenlp'\n",
    "\n",
    "def join_words(sentences, word_dict):\n",
    "    arr_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        ant = ''\n",
    "        new_sentence = ''\n",
    "        add_word = True\n",
    "        \n",
    "        for ix, token in enumerate(tokens):\n",
    "            if(add_word):\n",
    "                if(token == '-'  and ix > 0 and ix < (len(tokens) - 1)):\n",
    "                    join_word = tokens[ix-1] + '-' + tokens[ix+1]\n",
    "                    if word_dict.check(join_word):\n",
    "                        ant = join_word + ' '\n",
    "                        add_word = False\n",
    "\n",
    "                if(add_word):\n",
    "                    new_sentence += ant\n",
    "                    ant = token + \" \"\n",
    "            else:\n",
    "                add_word = True\n",
    "\n",
    "        new_sentence += ant\n",
    "        arr_sentences.append(new_sentence)\n",
    "    \n",
    "    return arr_sentences\n",
    "\n",
    "def remove_unnecesary_char(sentence):\n",
    "    sentence = sentence.strip(' ')\n",
    "    sentence = sentence.lstrip(')')\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def find_index(sentence):\n",
    "    words = sentence.split()\n",
    "    \n",
    "    lst_index = []\n",
    "    for ix_word, word in enumerate(words):\n",
    "        if '<head>' in word:\n",
    "            lst_index.append(ix_word)\n",
    "\n",
    "    return lst_index\n",
    "\n",
    "def find_trim_sentence(sentence, iz_del, der_del):\n",
    "    cont_iz = 0\n",
    "    seqs = []\n",
    "    seq = ''\n",
    "    tokens = sentence.split()\n",
    "    add = False\n",
    "    \n",
    "    for token in tokens:\n",
    "        if iz_del in token:\n",
    "            cont_iz += 1\n",
    "        \n",
    "        if cont_iz > 0:\n",
    "            add = True\n",
    "            seq += token + ' '\n",
    "        \n",
    "        if der_del in token and cont_iz:\n",
    "            cont_iz -= 1\n",
    "        \n",
    "        if cont_iz == 0 and add:\n",
    "            seq = seq.strip(' ')\n",
    "            seqs.append(seq)\n",
    "            seq = ''\n",
    "            add = False\n",
    "            \n",
    "    if cont_iz:\n",
    "        seqs.append(seq)\n",
    "            \n",
    "    return seqs\n",
    "\n",
    "def remove_LRB_RRB(sentences, iz_del, der_del):\n",
    "    arr_sentence = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        seqs = find_trim_sentence(sentence, iz_del, der_del)\n",
    "        remove = True\n",
    "        for seq in seqs:\n",
    "            arr = find_index(seq)\n",
    "            if len(arr):\n",
    "                sentence = seq.strip(iz_del + der_del + ' ')\n",
    "                remove = False\n",
    "                break\n",
    "\n",
    "        if remove:\n",
    "            sentence = re.sub(r'\\((.*?)\\)', '', sentence, re.DOTALL)\n",
    "        if ')' not in sentence.split()[-1]:\n",
    "            sentence = re.sub(r'(.*?)\\)', '', sentence, re.DOTALL)\n",
    "        else:\n",
    "            sentence = re.sub(r'\\)', '', sentence, re.DOTALL) \n",
    "\n",
    "        if '(' not in sentence.split()[0]:\n",
    "            sentence += ' .'\n",
    "            sentence = re.sub(r'\\((.*?)\\.', '', sentence, re.DOTALL) \n",
    "        else:\n",
    "            sentence = re.sub(r'\\(', '', sentence, re.DOTALL)     \n",
    "        arr_sentence.append(sentence)\n",
    "    \n",
    "    return arr_sentence\n",
    "\n",
    "def process_instance(file, word_target, text, word_dict, sense_ids=None, tokenizer=None, verbose=False):\n",
    "    pairs = []\n",
    "    sentences = []\n",
    "    \n",
    "    if not sense_ids:\n",
    "        sense_ids = re.findall(r'senseid=\\\"(.*?)\\\"', text, re.DOTALL)\n",
    "        is_test = False\n",
    "    else:\n",
    "        is_test = True\n",
    "        \n",
    "    context = re.findall(r'<context>(.*?)</context>', text, re.DOTALL)\n",
    "    word_ambiguos = re.findall(r'<head>(.*?)</head>', context[0], re.DOTALL)\n",
    "    \n",
    "    sentences = re.split(r'[\\.|:|?|!]', context[0])    \n",
    "    for sentence in sentences:\n",
    "        if '<head>' in sentence:\n",
    "            sentence = remove_unnecesary_char(sentence)     \n",
    "            prunes = remove_LRB_RRB([sentence], '(', ')')\n",
    "            \n",
    "            if(verbose):\n",
    "                print('---oracion')\n",
    "                print(sentence)\n",
    "                print('---oracion sin parentesis')\n",
    "                print(prunes[0])\n",
    "                print('\\n')\n",
    "                \n",
    "            for prune in prunes:\n",
    "                prune = join_words([prune], word_dict)[0] + ' .'\n",
    "                prune = re.sub('<head>', ' <head>', prune)\n",
    "                prune = re.sub('</head>', '</head> ', prune)\n",
    "                ann = tokenizer.annotate(prune)\n",
    "                prune = ' '.join([w.word for w in ann.sentence[0].token])\n",
    "                prune = re.sub('<head> ', ' <head>', prune)\n",
    "                prune = re.sub(' </head>', '</head> ', prune)\n",
    "                index_word = find_index(prune) \n",
    "\n",
    "                for sense_id in sense_ids:   \n",
    "                    pair = [[],[],[],[]]\n",
    "                    sense_id = re.sub(r'%|:', '', sense_id)\n",
    "                    pair[0] = ' '.join(re.sub(r'<head>(.*?)</head>', ' ' + word_ambiguos[0] + ' ', prune).split())\n",
    "                    pair[1] = ' '.join(re.sub(r'<head>(.*?)</head>', ' ' + word_target + '_' + sense_id + ' ', prune).split())\n",
    "                    pair[2] = word_target + '_' + sense_id\n",
    "                    pair[3] = index_word\n",
    "                    pairs.append(pair)\n",
    "                    \n",
    "                    # Saving in a file\n",
    "                    if not is_test:\n",
    "                        file.write(pair[0] + '\\t' + pair[1] + '\\n')\n",
    "                if is_test:\n",
    "                    index_word = str(index_word[0] if len(index_word) else index_word)\n",
    "                    file.write(pair[0] + '\\t' + index_word)\n",
    "                    for pair in pairs:\n",
    "                        file.write('\\t' + pair[2])\n",
    "                    file.write('\\n')\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "def load_senses(path_senses, path_test):\n",
    "    \n",
    "    targets_all = []\n",
    "    with open(path_test, 'r', encoding='iso-8859-1') as f:\n",
    "        xml = f.read()  \n",
    "    \n",
    "    instances = re.findall(r'<instance(.*?)</instance>', xml, re.DOTALL)\n",
    "    for ix_ins, instance in enumerate(instances):\n",
    "        data = '<instance' + instance + '</instance>'\n",
    "        senses_ids = re.findall(r'<head>', data, re.DOTALL)\n",
    "        targets_all.append(len(senses_ids))\n",
    "    \n",
    "    senses_all = []\n",
    "    with open(path_senses, 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        for line in lines:\n",
    "            senses = []\n",
    "            words = line.split()\n",
    "            for ix, word in enumerate(words):\n",
    "                if ix > 1:\n",
    "                    word = re.sub(r'%|:', '', word)\n",
    "                    senses.append(word)\n",
    "                    \n",
    "            senses_all.append(senses)\n",
    "    \n",
    "    return senses_all, targets_all\n",
    "\n",
    "def replace_characters(instance):\n",
    "    \n",
    "    data = '<instance' + instance + '</instance>'\n",
    "    data = re.sub(r'[^\\x20-\\x7E]', '', data)\n",
    "    data = re.sub(r' n\\'t', 'n\\'t', data)\n",
    "    data = re.sub(r'wou \\'d', 'uld', data)\n",
    "\n",
    "    data = re.sub(r' \\'re', ' are', data)\n",
    "    data = re.sub(r' \\'ve', ' have', data)\n",
    "\n",
    "    data = re.sub(r'it \\'s', 'it is', data)\n",
    "    data = re.sub(r'he \\'s', 'he is', data)\n",
    "    data = re.sub(r'i \\'m', 'i am', data)\n",
    "    data = re.sub(r'It \\'s', 'it is', data)\n",
    "    data = re.sub(r'He \\'s', 'he is', data)\n",
    "    data = re.sub(r'I \\'m', 'i am', data)\n",
    "\n",
    "    data = re.sub(r'\\[(.*?)\\]', '', data)\n",
    "    data = re.sub(r'&(.*?);', '', data)\n",
    "\n",
    "    data = re.sub(r' \\'d', 'd', data)\n",
    "    data = re.sub(r'&', '', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def construct_pairs(path_source, path_model, test_path = None, tokenizer = None, verbose=True, name_file='input.raw'):\n",
    "    \n",
    "    word_dict = enchant.Dict('en_US')\n",
    "    \n",
    "    with open(path_source, 'r', encoding='iso-8859-1') as f:\n",
    "        xml = f.read()\n",
    "\n",
    "    if test_path:\n",
    "        senses_all, _ = load_senses(test_path, path_source)    \n",
    "    \n",
    "    lexelts = re.findall(r'<lexelt(.*?)</lexelt>', xml, re.DOTALL)\n",
    "    pairs= []\n",
    "    ix_ins = 0\n",
    "    with open(os.path.dirname(path_source) + f'/{name_file}', 'w') as file:\n",
    "        for ix_lex, lexelt in enumerate(lexelts):\n",
    "            item = re.findall(r'item=\\\"(.*?)\\\"', lexelt, re.DOTALL)\n",
    "            word_target = item[0].split('.')[0]\n",
    "\n",
    "            instances = re.findall(r'<instance(.*?)</instance>', lexelt, re.DOTALL)\n",
    "            for instance in instances:\n",
    "                data = replace_characters(instance)\n",
    "                \n",
    "                context = re.findall(r'<context>(.*?)</context>', data, re.DOTALL)\n",
    "\n",
    "                if not test_path:\n",
    "                    pairs.extend(process_instance(file, word_target, data, word_dict, None, tokenizer, verbose))\n",
    "                else:\n",
    "                    pairs.extend(process_instance(file, word_target, data, word_dict, senses_all[ix_ins], tokenizer, verbose))\n",
    "                ix_ins += 1\n",
    "    return pairs\n",
    "\n",
    "def make_dirs(dirs):\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "            \n",
    "def dependency_parse(filepath,  client, tokenize=True):\n",
    "    print('\\nDependency parsing ' + filepath)\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    filepre = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    parentpath = os.path.join(dirpath, filepre + '.parents')\n",
    "    deps = []\n",
    "    with open(filepath) as file:\n",
    "        for line in tqdm(file, total=file.tell()):\n",
    "            temp = client.dependency_parse(line)\n",
    "            temp = list(map(lambda x: [int(x[1]), int(x[2])], temp))\n",
    "            temp = list(itertools.chain(*temp))\n",
    "            deps.append(temp)\n",
    "    np.save(parentpath, np.array(deps))\n",
    "\n",
    "def build_vocab(filepaths, dst_path, lowercase=True):\n",
    "    vocab = set()\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath) as f:\n",
    "            for line in f:\n",
    "                if lowercase:\n",
    "                    line = line.lower()\n",
    "                vocab |= set(line.split())\n",
    "    with open(dst_path, 'w') as f:\n",
    "        for w in sorted(vocab):\n",
    "            f.write(w + '\\n')\n",
    "\n",
    "def split(filepath, dst_dir, client, is_train=False):\n",
    "    with open(filepath) as datafile, \\\n",
    "            open(os.path.join(dst_dir, 'in.txt'), 'w') as afile, \\\n",
    "            open(os.path.join(dst_dir, 'out.txt'), 'w') as bfile:\n",
    "        datafile.readline()\n",
    "        for line in datafile:\n",
    "            a, b = line.strip().split('\\t')[:2]\n",
    "\n",
    "            afile.write(a + '\\n')\n",
    "            # if its test out.txt is a blank file\n",
    "            if is_train:\n",
    "                bfile.write(b + '\\n')\n",
    "\n",
    "def parse(dirpath, client):\n",
    "    dependency_parse(os.path.join(dirpath, 'in.txt'), client, tokenize=True)\n",
    "    #dependency_parse(os.path.join(dirpath, 'out.txt'), client, cp=cp, tokenize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Preprocessing dataset\n",
      "================================================================================\n",
      "Construction train data\n",
      "Construction test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data\n",
      "Parsing data\n",
      "\n",
      "Dependency parsing data/disambiguation/sense-eval2/train/in.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9080it [03:21, 44.97it/s]\n",
      "6it [00:00, 54.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency parsing data/disambiguation/sense-eval2/test/in.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4150it [01:36, 43.03it/s]\n",
      "4it [00:00, 32.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data\n",
      "Parsing data\n",
      "\n",
      "Dependency parsing data/disambiguation/sense-eval3/train/in.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8540it [03:03, 46.63it/s]\n",
      "6it [00:00, 53.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency parsing data/disambiguation/sense-eval3/test/in.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3943it [01:19, 49.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    print('=' * 80)\n",
    "    print('Preprocessing dataset')\n",
    "    print('=' * 80)\n",
    "    base_dir = ''\n",
    "    data_dir = os.path.join(base_dir, 'data')\n",
    "    lib_dir = os.path.join(data_dir, 'lib')\n",
    "    client_tree = StanfordCoreNLP(os.path.join(lib_dir, 'stanford-corenlp'))\n",
    "    client_tok = corenlp.CoreNLPClient(annotators=\"tokenize ssplit\".split())\n",
    "    \n",
    "    #Processing corpus train\n",
    "    print('Construction train data')\n",
    "    pairs = construct_pairs('data/disambiguation/sense-eval3/train/EnglishLS.train', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                              test_path=None, tokenizer=client_tok, verbose=False, name_file='train.raw')\n",
    "    pairs = construct_pairs('data/disambiguation/sense-eval2/train/eng-lex-sample.training.xml', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                              test_path=None, tokenizer=client_tok, verbose=False, name_file='train.raw')\n",
    "    #Processing corpus test\n",
    "    print('Construction test data')\n",
    "    pairs = construct_pairs('data/disambiguation/sense-eval3/test/EnglishLS.test', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                             test_path='data/disambiguation/sense-eval3/test/EnglishLS.test.key', tokenizer=client_tok, verbose=False, name_file='test.raw')\n",
    "    pairs = construct_pairs('data/disambiguation/sense-eval2/test/eng-lex-samp.evaluation.xml', '/home/krivas/projects/wsd-v2/data/lib/',\\\n",
    "                             test_path='data/disambiguation/sense-eval2/test/key.txt', tokenizer=client_tok, verbose=False, name_file='test.raw')\n",
    "    \n",
    "    senses_dir=['sense-eval2', 'sense-eval3']\n",
    "    for sense_dir in senses_dir:\n",
    "        train_dir = os.path.join(data_dir, f'disambiguation/{sense_dir}/train')\n",
    "        test_dir = os.path.join(data_dir, f'disambiguation/{sense_dir}/test')\n",
    "\n",
    "        # split into separate files\n",
    "        print('Splitting data')\n",
    "        split(os.path.join(train_dir, 'train.raw'), train_dir, client_tok, is_train=True)\n",
    "        split(os.path.join(test_dir, 'test.raw'), test_dir, client_tok)\n",
    "\n",
    "        # parse sentences\n",
    "        print('Parsing data')\n",
    "        parse(train_dir, client_tree)\n",
    "        parse(test_dir, client_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There is a tedious pounding at the heart of this music , a sincerity that allows for only one angle of delivery , a passion that has created a pop atmosphere in which it 's impossible to be clever , cool or cynical , a soul whose impulse is NOT to embrace the avant-garde , like Faustus , and be cut into a thousand pieces by a sampling machine .\",\n",
       " \"There is a tedious pounding at the heart of this music , a sincerity that allows for only one angle of delivery , a passion that has created a pop atmosphere in which it 's impossible to be clever , cool_cool30002 or cynical , a soul whose impulse is NOT to embrace the avant-garde , like Faustus , and be cut into a thousand pieces by a sampling machine .\",\n",
       " 'cool_cool30002',\n",
       " [40]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
